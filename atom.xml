<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CollinT</title>
  <subtitle>知我者，谓我心忧；不知我者，谓我何求。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.tangkailin.cn/"/>
  <updated>2018-01-28T15:03:17.831Z</updated>
  <id>http://blog.tangkailin.cn/</id>
  
  <author>
    <name>CollinT</name>
    <email>tkl449@126.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>推荐站点</title>
    <link href="http://blog.tangkailin.cn/2018/01/28/%E6%8E%A8%E8%8D%90%E7%AB%99%E7%82%B9/"/>
    <id>http://blog.tangkailin.cn/2018/01/28/推荐站点/</id>
    <published>2018-01-28T15:36:59.000Z</published>
    <updated>2018-01-28T15:03:17.831Z</updated>
    
    <content type="html"><![CDATA[<p>一些站点推荐。</p>
<a id="more"></a>
<h1 id="技术博客"><a href="#技术博客" class="headerlink" title="技术博客"></a>技术博客</h1><p><a href="https://blog.gushenzhou.com/" target="_blank" rel="external">觅风SuperShen</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些站点推荐。&lt;/p&gt;
    
    </summary>
    
      <category term="推荐" scheme="http://blog.tangkailin.cn/categories/%E6%8E%A8%E8%8D%90/"/>
    
    
      <category term="推荐" scheme="http://blog.tangkailin.cn/tags/%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>有些人是注定没有梦想的</title>
    <link href="http://blog.tangkailin.cn/2018/01/27/%E6%9C%89%E4%BA%9B%E4%BA%BA%E6%98%AF%E6%B3%A8%E5%AE%9A%E6%B2%A1%E6%9C%89%E6%A2%A6%E6%83%B3%E7%9A%84/"/>
    <id>http://blog.tangkailin.cn/2018/01/27/有些人是注定没有梦想的/</id>
    <published>2018-01-27T12:36:59.000Z</published>
    <updated>2018-01-28T14:35:56.244Z</updated>
    
    <content type="html"><![CDATA[<p>观《神秘巨星》。</p>
<a id="more"></a>
<p>有些人是注定没有梦想的，看到这句话，热泪盈眶。他们的梦想消磨在年复一年的承担责任；他们的梦想消磨在日复一日的照顾我们的日子里。有些道理，我们好像知道，却永远不会去重视；有些东西，我们心里知道，却永远不敢去面对和承认。面对我们的父母，我们心里知道要关心他们；知道要多陪伴他们；我们想到为他们买礼物；带他们做体检…… 却总会在某些时候去选择性的忽略他们。这是必须要面对的事实，只是你心里不敢承认。这或许是人性，人们总是同情弱者，崇拜强者。小时候我们崇拜父母，觉得他们是最强的依靠；长大后我们讨厌父母，觉得他们束缚了我们的翅膀；最终我们成为父母，才会知道父亲、母亲这两个称谓的分量。<br><br></p>

<p>主人公所处的家庭似乎是个无解的局，家暴而独裁的父亲，懦弱的母亲，还小的弟弟，老迈的奶奶。这一切，就这么过着。父亲因为一串项链、一顿忘记放盐的饭就会暴打母亲，奶奶永远默不作声，弟弟还小，只有主人公和母亲来承受这一切。主人公希望和母亲逃跑，但母亲却他们却无法离开父亲，离开父亲似乎会失去生活的来源和支柱。但不离开父亲就要承受压抑的人生。这似乎是很多“凑活着过”的家庭的状态，这是一个死局，你的一生很可能就在这个局里面死去。母亲其实清楚的认识到了这个点，但是为了子女，她甘愿死在这个局里。这一切都由奶奶最后的一番话来点明，这段话也为最后母亲的爆发埋下了伏笔。整部电影都处于一种压制的状态，死局一直存在着，但也不乏一些注入夏提克和胶带粘电脑这样的欢快的桥段，其间分寸拿捏得很好。<br><br></p>

<p>最后母亲的爆发，打破了死局。其实死局之所以成为死局，是因为局中的人。其实只要一点的努力，所有的局都是活局。母亲为了主人公入局，也为了主人公而出局。母亲注定没有梦想，她却成就了主人公的飞翔。热泪盈眶。</p>

<p>父亲却还在局中，父亲每天工作17小时。兢兢业业的工程师，希望能够获得更好的社会地位。他逃不开社会这个局。社会给他压力，令他麻木。他活在自己的传统中，树立着虚无的威信。这些暴力手段才能让他感觉自己还活着。他陶醉于自己取得的可怜的成绩，却从不质疑自己哪方面做得不够。他嫌弃母亲没文化，自命清高，却比谁都要庸俗。这其实是这个社会大多数人的常态，我看到过太多这样的人。他们是这个社会的牺牲品，他们接收教育，用自己的知识来服务这个社会。通过出卖自己的服务，换取可怜的报酬。他们没有梦想，他们的所谓梦想是被社会压抑的畸形梦想。他们也是可怜人啊。</p>

<p>阿米尔汗的电影，其实讲述的都是很简单的一个故事，反映一个好像大家都明白的道理。从印度这个包含了很多陈旧思想的国家的角度来说，实在是难能可贵。这部电影或许因为故事情节相对压抑的原因，感觉不如以前他的电影看着那么爽快 ，那么酣畅。但是，压抑的才是真实的人生啊。人生啊，就是由无数的遗憾组成的。<br><br></p>

<p><br><br><strong><em>为什么我们能站得更高，看得更远。因为我们站在巨人的肩膀上！You raise me up.</em></strong><br><br></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;观《神秘巨星》。&lt;/p&gt;
    
    </summary>
    
      <category term="日常随笔" scheme="http://blog.tangkailin.cn/categories/%E6%97%A5%E5%B8%B8%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="影评" scheme="http://blog.tangkailin.cn/tags/%E5%BD%B1%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>Running-mapreduce-job-into-Docker-container</title>
    <link href="http://blog.tangkailin.cn/2018/01/23/Running-mapreduce-job-into-Docker-container/"/>
    <id>http://blog.tangkailin.cn/2018/01/23/Running-mapreduce-job-into-Docker-container/</id>
    <published>2018-01-23T15:02:58.000Z</published>
    <updated>2018-01-24T02:37:11.491Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to contribute to  our <a href="https://github.com/ADSC-Resa/documentation" target="_blank" rel="external">documentation</a> on github.</p>
<a id="more"></a>
<p>The Hadoop 2.6 release contains a new feature that allows to launch Docker container directly as YARN container, called the DCE (Docker container executor). Running a Hadoop job is not an easy thing, User need to solve a lot of problem, such as complex problem of dependency. Using DCE let the developers package their applications and all of dependencies into a Docker container in order to provide a consistent environment for execution and also provides isolation from other applications installed on host. Because of the official reference document is relatively simple, and ignored a lot of details, this tutorial will describe the detail of DCE configuration and some problems needing attention.</p>
<h1 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h1><p>Your Hadoop version must be 2.7.i at minimum. The distro and version of Linux in your Docker image can be quite different from that of your Nodemanager. However, if you are using the MapReduce framework, then your Docker image will need to be configured for running Hadoop. Obviously, Java is in need, and the following environment variables must be defined in the image:  JAVA_HOME, HADOOP_COMMON_PATH, HADOOP_HDFS_HOME, HADOOP_MAPRED_HOME, HADOOP_YARN_HOME, and HADOOP_CONF_DIR.<br>Before running the DCE, please make sure that your host has installed Hadoop and Docker.</p>
<h1 id="Experimental-Environment"><a href="#Experimental-Environment" class="headerlink" title="Experimental Environment"></a>Experimental Environment</h1><p>You will have 3 machines running Ubuntu Server 14.04, each of which will be running a Docker daemon and Hadoop 2.7.3 inside. Besides we are using <a href="https://hub.docker.com/r/sequenceiq/hadoop-docker/" target="_blank" rel="external">sequenceiq/hadoop-docker2.4.1</a> image that developed by <a href="http://sequenceiq.com/" target="_blank" rel="external">sequenceiq</a>. This image already contains all of required environmental dependencies. Of course, you can also using your own Docker image</p>
<h1 id="Experimental-Procedure"><a href="#Experimental-Procedure" class="headerlink" title="Experimental Procedure"></a>Experimental Procedure</h1><p>##Step 1：Pull the Docker image</p>
<p>You can use the following command to pull image:</p>
<blockquote>
<p>docker pull sequenceiq/hadoop-docker:2.4.1 </p>
</blockquote>
<p>and, you can type the following command to inspect the running environment into the Docker container:</p>
<blockquote>
<p>docker run -it sequenceiq/hadoop-docker:2.7.1 /etc/bootstrap.sh -bash  </p>
</blockquote>
<p>If nothing has gone wrong, you should now find the Hadoop directory into /usr/local that inside the container. </p>
<h2 id="Step-2：Configure-the-yarn-site-xml-on-the-host"><a href="#Step-2：Configure-the-yarn-site-xml-on-the-host" class="headerlink" title="Step 2：Configure the yarn-site.xml on the host"></a>Step 2：Configure the yarn-site.xml on the host</h2><p>You need to add the following contents into yarn-site.xml:</p>
<blockquote>
<p>\<property\><br>      \<name\>yarn.nodemanager.docker-container-executor.exec-name\</name\><br>       \<value\>/usr/bin/docker\</value\><br>\</property\><br>\<property\><br>       \<name\>yarn.nodemanager.container-executor.class\</name\><br>       \<value\>org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor\</value\><br>\</property\>  </p>
</blockquote>
<p>The first configuration items <em>yarn.nodemanager.docker-container-executor.exec-name</em> is configure the path of the Docker executable file. And the second items <em>yarn.nodemanager.docker-container-executor.class</em> is configure the DockerContainerExecutor as container executor rather than the default container executor into YARN. </p>
<h2 id="Step-3：Restart-the-YARN-and-HDFS-on-the-host"><a href="#Step-3：Restart-the-YARN-and-HDFS-on-the-host" class="headerlink" title="Step 3：Restart the YARN and HDFS on the host"></a>Step 3：Restart the YARN and HDFS on the host</h2><blockquote>
<p>./sbin/stop-all.sh<br>./sbin/start-all.sh </p>
</blockquote>
<h2 id="Step-4：Submit-a-MapReduce-job"><a href="#Step-4：Submit-a-MapReduce-job" class="headerlink" title="Step 4：Submit a MapReduce job"></a>Step 4：Submit a MapReduce job</h2><p>Here, we are using the hadoop-mapreduce-examples-2.7.3.jar π calculation.<br>Type following command:</p>
<blockquote>
<p>bin/Hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi<br>-D mapreduce.map.env=”yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.4.1”<br>-D mapreduce.reduce.env=”yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.4.1”<br>-D yarn.app.mapreduce.am.env=”yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.4.1” 5 10</p>
</blockquote>
<p>Currently you cannot configure any of the Docker settings with the job configuration. You can provide Mapper, Reducer, and ApplicationMaster environment overrides for the docker images, using the following 3 JVM properties respectively(only for MR jobs):<br>-mapreduce.map.env: You can override the mapper’s image by passing yarn.nodemanager.docker-container-executor.image-name=your_image_name to this JVM property.<br>-mapreduce.reduce.env: You can override the reducer’s image by passing yarn.nodemanager.docker-container-executor.image-name=your_image_name to this JVM property.<br>-yarn.app.mapreduce.am.env: You can override the ApplicationMaster’s image by passing yarn.nodemanager.docker-container-executor.image-name=your_image_name to this JVM property.</p>
<p>If noting has gone wrong, you can <em>docker ps</em> command to affirm Docker container is running. And you can find that the name of Docker container is same as the default YARN container name that show on the terminal. </p>
<h1 id="Need-to-be-aware-of-problem-in-the-experiment"><a href="#Need-to-be-aware-of-problem-in-the-experiment" class="headerlink" title="Need to be aware of problem in the experiment"></a>Need to be aware of problem in the experiment</h1><ol>
<li>Don`t use Hadoop 3.0.0 or higher version, There will be some puzzling error.</li>
<li>Error: No such image or container<br>This error may be because of the Docker image version do not match the Hadoop version on the host. And you can refer to log file of Nodemanager.</li>
<li>Diagnostics: Container image must not be null<br>Two reasons led to this error:<br>A.  The Hadoop running no hosts do not support using DCE as YARN container.<br>B.  The configuration of YARN is wrong or when you submit job, you forget configure for DCE.<br>Note. DCE and default YARN container can’t be use at same time in cluster. </li>
<li>Error: No such image , container or task<br>This error is not that simple. You can refer to error log file (stderr) of MapReduce job. If you find message about <em>Error: Could not find or load main class org.apache.-hadoop.mapreduce.v2.app.MRAppMaster</em>. You can add the library path that is inside the Docker container to the classpath of MapReduce job in mapred-site.xml. If you find message about <em>Exception in thread “main” java.lang.NoClassDefFoundError:-org/apache/hadoop/service/CompositeService</em>. You need also add the library path that is inside the Docker container to the classpath of YARN in yarn -site.xml.</li>
<li>The MR job is stuck in the accepted.<br>You can refer to log file of Nodemanager. This error maybe result from resource that schedule by YARN is not enough, then you need to increase the number of resource in yarn-site.xml. May also result from the Docker container can’t get the PID, at this time, please read this <a href="http://blog.sequenceiq.com/blog/2015/01/07/yarn-containers-docker/" target="_blank" rel="external">article</a> seriously.  </li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to contribute to  our &lt;a href=&quot;https://github.com/ADSC-Resa/documentation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;documentation&lt;/a&gt; on github.&lt;/p&gt;
    
    </summary>
    
      <category term="大数据学习总结" scheme="http://blog.tangkailin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Distributed system" scheme="http://blog.tangkailin.cn/tags/Distributed-system/"/>
    
      <category term="Map Reduce" scheme="http://blog.tangkailin.cn/tags/Map-Reduce/"/>
    
      <category term="Docker" scheme="http://blog.tangkailin.cn/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Deploy-storm-on-docker</title>
    <link href="http://blog.tangkailin.cn/2018/01/23/deploy-storm-on-docker/"/>
    <id>http://blog.tangkailin.cn/2018/01/23/deploy-storm-on-docker/</id>
    <published>2018-01-23T14:59:09.000Z</published>
    <updated>2018-01-26T14:43:28.311Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to contribute to  our <a href="https://github.com/ADSC-Resa/documentation" target="_blank" rel="external">documentation</a> on github.</p>
<a id="more"></a>
<p>This guide reference <a href="http://highscalability.com/blog/2016/4/25/the-joy-of-deploying-apache-storm-on-docker-swarm.html" target="_blank" rel="external">The joy of deploying Apache Storm on Docker Swarm</a> in <a href="http://www.baqend.com/" target="_blank" rel="external">Baqend Tech</a>.And we describe some of the detail and principles about every step in this tutorial.If you have already have a good use of Linux,Docker swarm and Apache storm,you can use script given in the article above for rapid deployment. This tutorial is targeted at beginner novice.We will manual configuration on each machine and you will be more clear understand the principle in the process of deployment .</p>
<p>So let’s begin :</p>
<h1 id="Overall-Architecture"><a href="#Overall-Architecture" class="headerlink" title="Overall Architecture"></a>Overall Architecture</h1><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/ADSC-Resa/documentation/wiki/1.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>You’ll have 3 machines running Ubuntu Server 14.04, each of which will be running a Docker daemon with several containers inside. As shown in figure above ,we use ubuntu1 as manager of Docker swarm. The Nimbus and UI containers will be spawned on the manager node (Ubuntu 1).Beside ,remember to open port 8080 for UI container.<br>When swarm is in place ,you’ll create an overlay network (stormnet) to enable communication between Docker container hosted on the different swarm nodes. Finally, you will set up a full-fledged storm cluster that uses the existing zookeeper ensemble for coordination and stormnet for inter-node communication. This involves discovery service of Docker swarm ,you can read <a href="https://technologyconversations.com/2015/09/08/service-discovery-zookeeper-vs-etcd-vs-consul/" target="_blank" rel="external">here</a>.<br>We are using hostnames dmir1,dmir2 and dmir3 for the three Ubuntu machines. Using alias rather than an IP address can have better fault tolerance. At the time of the actual deployment ,remember to replace your own domain name. Three zookeeper severs are deployed here, of course,using one zookeeper is also possible. </p>
<h1 id="Install-Docker"><a href="#Install-Docker" class="headerlink" title="Install Docker"></a>Install Docker</h1><p>Using your favorite way to connect three machines and install Docker on each machine. You can reference <a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/" target="_blank" rel="external">here</a> to install Docker. After the installation is complete,we should test to ensure that the installation is successful.</p>
<h1 id="Create-etc-init-sh"><a href="#Create-etc-init-sh" class="headerlink" title="Create /etc/init.sh"></a>Create /etc/init.sh</h1><p>Create the file used to configure the Docker swarm worker.</p>
<p>In the terminal ,type :</p>
<blockquote>
<p>sudo touch /etc/init.sh<br>sudo vim /etc/init.sh</p>
</blockquote>
<p>and then paste the following and save:</p>
<blockquote>
<p>#!/bin/bash<br># first script argument: the servers in the ZooKeeper ensemble:<br>ZOOKEEPER_SERVERS=$1<br># second script argument: the role of this node:<br># (“manager” for the Swarm manager node; leave empty else) ROLE=$2 # the IP<br>address of this machine:<br>PRIVATE_IP=$(/sbin/ifconfig eth0 | grep ‘inet addr:’ | cut -d: -f2 | awk ‘{ print &gt;$1}’)<br># define label for the manager node: if [[ $ROLE == “manager” ]];then LABELS=”–<br>label server=manager”;else LABELS=””;fi<br># define default options for Docker Swarm:<br>echo “DOCKER_OPTS=\”-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock –cluster&gt;-advertise eth0:2375 $LABELS –cluster-store zk://$ZOOKEEPER_SERVERS\”” | sudo<br>tee /etc/default/docker<br># restart the service to apply new options:<br>sudo service docker restart<br>echo “let’s wait a little…”<br>sleep 10<br># make this machine join the Docker Swarm cluster:<br>docker run -d –restart=always swarm join –advertise=$PRIVATE_IP:2375<br>zk://$ZOOKEEPER_SERVERS  </p>
</blockquote>
<p>In this shell file, we configure some detail for start a swarm worker. The first step , we defined three zookeeper server node ,through the ZOOKEEPER_SERVERS variable. The second step, we parse <em>/sbin/ifconfig eth0</em> to get the IP address of the machine($PRIVATE_IP). The third step, we mark the label “manager ” for swarm manager node. The fourth step, modify <em>/ect/default/docker</em>, including some content such as docker daemon listening port 2375,which can refer to the configuration of the docker on the <a href="https://docs.docker.com/" target="_blank" rel="external">https://docs.docker.com/</a>. The fifth step, restart Docker service. The last step, <em>docker run</em> a swarm worker container ,and use zookeeper as the service discovery.  </p>
<h1 id="Create-swarm-worker"><a href="#Create-swarm-worker" class="headerlink" title="Create swarm worker"></a>Create swarm worker</h1><p>In the terminal of Ubuntu1 ,type :</p>
<blockquote>
<p>/bin/bash /etc/init.sh dmir1,dmir2,dmir3  manager</p>
</blockquote>
<p>In the terminal of Ubuntu2 and Ubuntu3, type :</p>
<blockquote>
<p>/bin/bash /etc/init.sh dmir1,dmir2,dmir3</p>
</blockquote>
<p>set up your DNS in such a way that the first hostname in the list (dmir1) points towards the manager on Ubuntu 1 and the other two hostnames (dmir2 and dmir3) point towards the other two machines, i.e. Ubuntu 2 and Ubuntu 3.<br>Configure your security settings to allow connections between the machines on ports 2181, 2888, 3888 (zookeeper), 2375 (Docker Swarm) and 6627 (Storm, remote topology deployment).<br>If nothing has gone wrong, you should now have three Ubuntu servers, each running a Docker daemon. Ubuntu 1 should be reachable via dmir1 in your private network. Now, we can only configuration on Ubuntu1, it is going to be the only machine you will talk to from this point on.</p>
<h1 id="Start-Docker-swarm-cluster-and-zookeeper-server"><a href="#Start-Docker-swarm-cluster-and-zookeeper-server" class="headerlink" title="Start Docker swarm cluster and zookeeper server"></a>Start Docker swarm cluster and zookeeper server</h1><p>Using <em>docker ps</em> to perform a quick heal check. If Docker is installed correctly, the terminal will show a list of the running Docker container (exactly 1 for swarm and nothing else ) </p>
<p>You are now good to launch one zookeeper node on every machine like this:</p>
<blockquote>
<p>docker -H tcp://dmir1:2375 run -d –restart=always -p 2181:2181 -p 2888:2888 -p<br>3888:3888 -v /var/lib/zookeeper:/var/lib/zookeeper -v /var/log/zookeeper:/var/log/zookeeper –name zk1 baqend/zookeeper dmir1,dmir2,dmir3 1<br>docker -H tcp://dmir2:2375 run -d –restart=always -p 2181:2181 -p 2888:2888 -p 3888:3888 -v /var/lib/zookeeper:/var/lib/zookeeper -v /var/log/zookeeper:/var/log/zookeeper –name zk2 baqend/zookeeper dmir1,dmir2,dmir3 2<br>docker -H tcp://dmir3:2375 run -d –restart=always -p 2181:2181 -p 2888:2888 -p 3888:3888 -v /var/lib/zookeeper:/var/lib/zookeeper -v /var/log/zookeeper:/var/log/zookeeper –name zk3 baqend/zookeeper dmir1,dmir2,dmir3 3  </p>
</blockquote>
<p>Here we using <a href="https://hub.docker.com/r/baqend/zookeeper/" target="_blank" rel="external">baqend/zookeeper</a> image on the docker hub.<br>By specifying the -H … argument, we are able to launch the zookeeper containers on the different host machines. The -p commands expose the ports required by zookeeper per default. The two -v commands provide persistence in case of container failure by mapping the directories the zookeeper container uses to the corresponding host directories. The comma-separated list of hostnames tells zookeeper what servers are in the ensemble. This is the same for every node in the ensemble. The only variable is the zookeeper ID (second argument), because it is unique for every container.    </p>
<p>To check zookeeper health, you can do the following:</p>
<blockquote>
<p>docker -H tcp://dmir1:2375 exec -it zk1 bin/zkServer.sh status &amp;&amp; docker -H<br>tcp://dmir2:2375 exec -it zk2 bin/zkServer.sh status &amp;&amp; docker -H<br>tcp://dmir3:2375 exec -it zk3 bin/zkServer.sh status</p>
</blockquote>
<p>Here related to Docker exec command and the zookeeper service status command bin/zkServer.sh status. If your cluster is healthy, every node will report whether it is the leader or one of the followers. If something goes wrong,likely due to $PRIVATE_IP can’t be parsed, then we recommended that removed the last step in the init.sh and manually <em>docker run</em> a swarm worker on each node.</p>
<blockquote>
<p>docker run -d –restart=always swarm join –advertise=$PRIVATE_IP:2375<br>zk://$ZOOKEEPER_SERVERS</p>
</blockquote>
<p>After you start the zookeeper service ,you need to start the swarm manager:</p>
<blockquote>
<p>docker run -d –restart=always –label role=manager -p 2376:2375 swarm<br>manage zk://dmir1,dmir2,dmir3</p>
</blockquote>
<p>Now the Swarm cluster is running. However, we still have to tell the Docker client about it. So finally, you only have to make sure that all future docker run statements are directed to the Swarm manager container (which will do the scheduling) and not against the local Docker daemon:</p>
<blockquote>
<p>cat &lt;&lt; EOF | tee -a ~/.bash_profile<br># this node is the master and therefore should be able to talk to the Swarm<br>cluster:<br>export DOCKER_HOST=tcp://127.0.0.1:2376<br>EOF</p>
</blockquote>
<p>This will do it for the current session and also make sure it will be done again when we log into the machine next time.<br>Now everything should be up and running. Type in <em>docker info</em> to check cluster status on the manager node. </p>
<p>You should see 3 running workers similar to this:</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/ADSC-Resa/documentation/wiki/2.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>The important part is the line with Status: Healthy for each node. If you observe something like Status: Pending or if not all nodes show up, even though you are not experiencing any errors elsewhere, try restarting the manager container like so:</p>
<blockquote>
<p>docker restart $(docker ps -a –no-trunc –filter “label=role=manager)</p>
</blockquote>
<h1 id="Setup-the-storm-cluster"><a href="#Setup-the-storm-cluster" class="headerlink" title="Setup the storm cluster"></a>Setup the storm cluster</h1><p>First, create the overlay network stormnet. This network is guaranteed the mapping relationship between container and physical machine.you can reference <a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/" target="_blank" rel="external">here</a>.</p>
<blockquote>
<p>docker network create –driver overlay stormnet<br>docker network ls  </p>
</blockquote>
<p>This creates a network and detect if created successfully. If something goes wrong,maybe your swarm cluster is not running. It means that you should refer to the previous tutorial and guarantee every step is carefully finished.<br>Now, let’s start storm components, including UI, nimbus and supervisor.</p>
<p>First, start the UI:</p>
<blockquote>
<p>docker run -d –label cluster=storm –label role=ui -e constraint:server==manager -e STORM_ZOOKEEPER_SERVERS=dmir1,dmir2,dmir3 –net stormnet –restart=always –name ui -p 8080:8080 baqend/storm ui</p>
</blockquote>
<p>and the nimbus :</p>
<blockquote>
<p>docker run -d –label cluster=storm –label role=nimbus -e<br>constraint:server==manager -e STORM_ZOOKEEPER_SERVERS=dmir1,dmir2,dmir3 –net<br>stormnet –restart=always –name nimbus -p 6627:6627 baqend/storm nimbus  </p>
</blockquote>
<p>For specific configuration can refer to <a href="https://hub.docker.com/r/baqend/storm/" target="_blank" rel="external">baqend/storm</a>. The -p commands expose the ports required by UI container and nimbus container default. The -net command specified the container under the stormnet, in order to communicate with the container on other physical machine.<br>Note that nimbus parameter are added after baqend/storm, when we start nimbus container. This parameter is mark the nimbus container as the main container. This involves some  configuration of baqend/storm, you can view <a href="https://github.com/Baqend/docker-storm" target="_blank" rel="external">here</a>.<br>To make sure that these are running on the manager node, we specified a <em>constraint : constraint : server==manager</em>. You can now access the Storm UI as though it would be running on the manager node, However, there are no supervisors running, yet.</p>
<p>Finally, start the supervisor, you can start supervisor on any machine. </p>
<blockquote>
<p>docker run -d –label cluster=storm –label role=supervisor -e<br>affinity:role!=supervisor -e STORM_ZOOKEEPER_SERVERS=dmir1,dmir2,dmir3 –net<br>stormnet –restart=always baqend/storm supervisor -c supervisor.slots.ports=[6700]</p>
</blockquote>
<p>Since we do not care where exactly the individual supervisors are running, we did not specify any constraints or container names here. However, in order to prevent two supervisors from being hosted on one machine, we did specify a label <em>affinity : affinity : role!=supervisor</em>. If you need more supervisor containers, you’ll have to add additional Swarm worker nodes (Ubuntu 4, Ubuntu 5, …). Besides, the <em>-c supervisor.slots.ports=[6700]</em> provided one port 6700 for supervisor. Of course you can provide any number of port for supervisor. But we recommend that you use only one port, because the container is essentially a process on physical machine , no matter how much the port resources are the same.<br>Have a look at the Storm UI and make sure that you have supervisors running.</p>
<h1 id="Topology-Deployment"><a href="#Topology-Deployment" class="headerlink" title="Topology Deployment"></a>Topology Deployment</h1><p>Deploying a topology can now be done from any server that has a Docker daemon running and is in the same network as the manager machine. The following command assumes that your topology fatjar is a file called topology.jar in your current working directory:</p>
<blockquote>
<p>docker -H tcp://127.0.0.1:2375 run -it –rm -v $(readlink -m topology.jar)<br>:/topology.jar –net stormnet baqend/storm jar /topology.jar main.class arg1 arg2</p>
</blockquote>
<p>This command will spawn a Docker container, deploy the topology and then remove the container. You should provide the <em>-H tcp://127.0.0.1:2375</em> argument to make sure the container is started on the machine you are currently working on; if you left the scheduling to Docker Swarm, the deployment might fail because the spawning host does not necessarily have the topology file.By the way, we use <em>readlink -m topology.jar</em> which produces an absolute path fortopology.jar, because relative paths are not supported. You can also provide an absolute path directly, though.<br>Last but most important , <em>–net stormnet</em> is necessary, because you spawn a new container for deploy the topology, so this new container must under the stormnet ,otherwise it will return a <em>unknownhost error</em>.</p>
<h1 id="Killing-a-Topology"><a href="#Killing-a-Topology" class="headerlink" title="Killing a Topology"></a>Killing a Topology</h1><p>Killing the topology can either be done via the Storm web UI interactively or, assuming the running topology is called runningTopology, like this:</p>
<blockquote>
<p>docker run -it –rm –net stormnet baqend/storm  kill runningTopology</p>
</blockquote>
<p>The host argument -H … is not required here, because the statement stands on its own and has no file dependencies.</p>
<h1 id="Topology-Rebalance"><a href="#Topology-Rebalance" class="headerlink" title="Topology Rebalance"></a>Topology Rebalance</h1><p>Topology rebalance can either be done via the Storm web UI interactively or, assuming the running topology is called runningTopology, like this:</p>
<blockquote>
<p>docker run -it –rm –net stormnet baqend/storm  rebalance runningTopology -n arg</p>
</blockquote>
<p>or:</p>
<blockquote>
<p>docker exec -it nimbus storm rebalance runningTopology -n arg</p>
</blockquote>
<p>Of course , we need enough slot to support this operation.</p>
<h1 id="Shutting-down-the-storm-cluster"><a href="#Shutting-down-the-storm-cluster" class="headerlink" title="Shutting down the storm cluster"></a>Shutting down the storm cluster</h1><p>Since every Storm-related container is labelled with cluster=storm, you can kill all of them with the following statement:</p>
<blockquote>
<p>docker rm -f $(docker ps -a –no-trunc –filter “label=cluster=storm”</p>
</blockquote>
<p>Finally, we finish our work. we stepped over how to <a href="https://docs.docker.com/swarm/configure-tls/" target="_blank" rel="external">configure Docker Swarm for TLS</a>. If you are planning to use Docker Swarm in a business-critical application, you should definitely put some effort into this aspect of deployment.</p>
<p>You can use our <a href="https://github.com/wendyshusband/storm-On-Docker-Swarm" target="_blank" rel="external">script</a> for repid depolyment</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to contribute to  our &lt;a href=&quot;https://github.com/ADSC-Resa/documentation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;documentation&lt;/a&gt; on github.&lt;/p&gt;
    
    </summary>
    
      <category term="Storm知识" scheme="http://blog.tangkailin.cn/categories/Storm%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Distributed system" scheme="http://blog.tangkailin.cn/tags/Distributed-system/"/>
    
      <category term="Docker" scheme="http://blog.tangkailin.cn/tags/Docker/"/>
    
      <category term="Apache Storm" scheme="http://blog.tangkailin.cn/tags/Apache-Storm/"/>
    
  </entry>
  
  <entry>
    <title>Storm中的分布式缓存</title>
    <link href="http://blog.tangkailin.cn/2018/01/20/Storm%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"/>
    <id>http://blog.tangkailin.cn/2018/01/20/Storm中的分布式缓存/</id>
    <published>2018-01-20T02:50:56.000Z</published>
    <updated>2018-01-26T14:43:28.311Z</updated>
    
    <content type="html"><![CDATA[<p> Storm中的分布式缓存<br><a id="more"></a></p>
<p><br>原文地址：<a href="http://storm.apache.org/releases/1.1.1/distcache-blobstore.html" target="_blank" rel="external">http://storm.apache.org/releases/1.1.1/distcache-blobstore.html</a><br><br></p>

<p>翻译水平有限，欢迎各位指正。</p>

<h2 id="Storm-分布式缓存API"><a href="#Storm-分布式缓存API" class="headerlink" title="Storm 分布式缓存API"></a>Storm 分布式缓存API</h2><p>    Storm中的分布式缓存的主要用途在于存储那些在topology生命周期中会产生变化且数量众多的文件（即blobs,在本文档中，blobs和“分布式缓存中文件”的意义等价），如位置数据、字典数据等。blobs的典型的应用场景包括短语识别、实体提取、文档分类、URL地址重写、定位/地址检测等。这些blobs数据的大小从几KB到几GB不等。对于那些不会动态更新的小数据集合，我们将其直接打包在topology的jar包中是可行的，但是对于那些大型的数据集合，打包再提交的启动时间将会非常大。在这个例子中，使用分布式缓存能大大提高topology的启动速度，特别要指出的是，同一个submitter提交的文件，会一直驻留在缓存中。这样的设计使得缓存中的数据可以重复利用。<br><br></p>

<p>在topology启动时，用户指定好哪些文件是topology所需要的。topology开始运行后，用户可以随时请求blobs并且更新其到新的版本。blobs的更新基于最终一致性模型（eventual consistency model）。如果topology想要知道其访问的文件的具体版本，这需要用户自己来实现相应信息的查询功能。缓存文件的置换使用 Least-Recently Used (LRU)算法，supervisor将基于这一算法对缓存文件进行置换。此外，blobs是可以压缩的，用户可以在使用其之前将其解压。<br><br></p>

<h2 id="使用分布式缓存的动机"><a href="#使用分布式缓存的动机" class="headerlink" title="使用分布式缓存的动机"></a>使用分布式缓存的动机</h2><ul>
<li>允许在topology之间共享blobs。</li>
<li>允许从命令行对blob进行更新。</li>
</ul>
<h2 id="分布式缓存的实现"><a href="#分布式缓存的实现" class="headerlink" title="分布式缓存的实现"></a>分布式缓存的实现</h2><p>目前的BlobStore包括两个实现：LocalFsBlobStore 和HdfsBlobStore。详细接口请参考附录A。</p>

<p>###LocalFsBlobStore </p>
<p><img src="http://storm.apache.org/releases/1.1.1/images/local_blobstore.png" alt=""></p>
<p>Blobstore的本地文件系统实现如上面的时间线图中所示。<br><br>blob的使用包括创建blob、下载blob以及在topology中使用blob。主要的步骤如下所示：<br><br>####创建Blob的命令<br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1</div></pre></td></tr></table></figure><br><br>上面的命令创建了一个名为“key1”的blob，对应于文件readme.txt，对所有用户的访问权限为读(r)、写(w)、管理(a)。此外，该文件包含4个拷贝。<br><br>#### topology提交及Blob映射<br><br>用户可以使用下面的命令来提交topology。该命令包括拓扑映射配置。该配置包含两个键“key1”和“key2”，其中键“key1”具有一个名为“blob_file”的没有被压缩的本地文件名映射。<br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar </div><div class="line">org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='&#123;"key1":&#123;"localname":"blob_file", "uncompress":"false"&#125;,"key2":&#123;&#125;&#125;'</div></pre></td></tr></table></figure><br><br>####Blob创建进程<br>Blob通过ClientBlobStore接口进行创建。附录B中包含了ClientBlobStore中的所有接口。ClientBlobStore的一个具体实现是NimbusBlobStore。在使用本地文件系统的情况下，客户端调用nimbus来创建本地文件系统中的blob。nimbus使用本地文件系统实现来创建这些blob。当用户提交一个topology，包括jar、配置文件以及代码文件都被作为blob提交到blobstore中。<br><br>####Supervisor下载blob<br>最终，在topology运行时，同一个NimbusBlobStore thrift客户端上传的blob,通过nimbus分配到指定的supervisor，supervisor接受到分配指令后，即会下载对应的blob。supervisor通过NimbusBlobStore client直接下载topology的jar、conf blobs等。<br><br></p>

<p>###HdfsBlobStore<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://storm.apache.org/releases/1.1.1/images/hdfs_blobstore.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>HdfsBlobStore 的blob创建和下载过程的实现和Local file system上的实现类似，唯一的区别在于支持对blobstore的多个拷贝。实现数据的多个拷贝是HDFS的天生技能，这也使得该模式下，我们不需要把blob的状态存储都在zookeeper中。另一方面，本地文件系统的blobstore需要将状态存储在zookeeper中，以便能和nimbus HA一起协同工作。Nimbus HA允许本地文件系统无缝地实现多拷贝这一特性，将状态存储在zookeeper的运行topology数据中，并在不同的Nimbus上同步这些blob。改模式下，最终supervisor使用HdfsClientBlobStore来与HdfsBlobStore 进行通信。</p>

<p>##其他特性及相关文档</p>
<p><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo </div><div class="line">-c topology.blobstore.map='&#123;"key1":&#123;"localname":"blob_file", "uncompress":"false"&#125;,"key2":&#123;&#125;&#125;'</div></pre></td></tr></table></figure><br><br>###压缩<br>BlobStore允许用户将<em>uncompress</em>配置项指定为true或false。这个配置可以在topology.blobstore.map中指定。上面的命令表示允许用户上传压缩文件，如tar/zip。在本地文件系统blobstore中，压缩的blob存储在nimbus节点上。本地化代码负责对blob进行解压，并将其存储在supervisor节点上。在用户逻辑开始执行之前，supervisor节点上blob的符号链接（Symbolic links）将会在worker中被创建。<br><br>###本地文件名称映射<br><br>除了压缩之外，在不同的supervisor节点上，本地化器(localizer)可在将blob映射为一个本地化的名称。<br><br>###BlobStore的一些具体实现细节<br><br>BlobStore基于哈希函数创建blobs. Blob通常存储在BlobStore指定的目录中（配置项blobstore.dir）。默认配置为<em>storm.local.dir/blobs</em><br><br>一旦提交了一个文件，BlobStore将读取configs，并为blob创建一个带有所有访问控制细节的元数据。在访问blob时，元数据通常用于授权。blob的哈希值基于blob数据的key和版本号生成。blob数据默认放置在<em>storm.local.dir/blobs/data</em>目录下。通常会生成放置在一个正数命名的目录来放置，如193,822。<br><br>一旦topology启动，storm.conf, storm.ser和storm.code相关的blobs将会首先被下载，其他的命令行上上传的所有blob都使用本地化器来解压，并将它们映射到<em>topology.blobstore.map</em>指定的本地名称。supervisor通过检查版本的变化定期更新blob。动态地更新blob使它成为一个非常有用的特性。<br><br>对于本地文件系统，supervisor节点上的分布式缓存被设置为1024mb（软限制），同时，根据LRU策略，将会在每600秒内清除任何超过软限制的内容。<br><br>另一方面，HDFS BlobStore的实现通过消除在nimbus上存储blob的负担来更好地处理负载，从而避免它成为一个瓶颈。此外，它还提供了blob的无缝拷贝，本地文件系统BlobStore在复制blob时效率不高，并且受到nimbus的数量的限制。此外，在没有使用nimbus存储blob的情况下，supervisor直接与HDFS BlobStore进行通信，从而减少了对nimbus的负载和依赖性。<br><br></p>

<h2 id="高可用的Nimbus"><a href="#高可用的Nimbus" class="headerlink" title="高可用的Nimbus"></a>高可用的Nimbus</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>目前，nimbus是一个在单个机器上运行的进程。在大多数情况下，nimbus失败是暂时的，并且可以通过执行监督的进程来重新启动。然而，当磁盘出现故障或者网络分区出现时，nimbus就会宕机。这种情况下，已经启动的topology能够正常运行，但是无法提交新的topology，且无法对运行着的topology进行kill、deactivated或activated等操作。此时，如果supervisor节点出现故障，则不会执行topology资源重新分配，从而导致性能下降或topology故障。当前的解决方案是，启动多台nimbus来避免单点故障。</p>

<h3 id="高可用Nimbus的要求"><a href="#高可用Nimbus的要求" class="headerlink" title="高可用Nimbus的要求"></a>高可用Nimbus的要求</h3><ul>
<li>增加nimbus的总体可用性。</li>
<li>允许nimbus主机随时离开并加入集群。一个新加入的主机应该自动加入可用的nimbus名单。</li>
<li>在nimbus失败的情况下，不需要进行拓扑重新提交。</li>
<li>任何正在执行的topology都不应丢失。</li>
</ul>
<h4 id="集群领导选举（Leader-Election）"><a href="#集群领导选举（Leader-Election）" class="headerlink" title="集群领导选举（Leader Election）"></a>集群领导选举（Leader Election）</h4><p>nimbus包含以下接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ILeaderElector</span> </span>&#123;</div><div class="line">    <span class="comment">/**</span></div><div class="line"><span class="comment">     * queue up for leadership lock. The call returns immediately and the caller                     </span></div><div class="line"><span class="comment">     * must check isLeader() to perform any leadership action.</span></div><div class="line"><span class="comment">     */</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addToLeaderLockQueue</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line"><span class="comment">     * Removes the caller from the leader lock queue. If the caller is leader</span></div><div class="line"><span class="comment">     * also releases the lock.</span></div><div class="line"><span class="comment">     */</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">removeFromLeaderLockQueue</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line"><span class="comment">     *</span></div><div class="line"><span class="comment">     * <span class="doctag">@return</span> true if the caller currently has the leader lock.</span></div><div class="line"><span class="comment">     */</span></div><div class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isLeader</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line"><span class="comment">     *</span></div><div class="line"><span class="comment">     * <span class="doctag">@return</span> the current leader's address , throws exception if noone has has    lock.</span></div><div class="line"><span class="comment">     */</span></div><div class="line">    <span class="function">InetSocketAddress <span class="title">getLeaderAddress</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line"><span class="comment">     * </span></div><div class="line"><span class="comment">     * <span class="doctag">@return</span> list of current nimbus addresses, includes leader.</span></div><div class="line"><span class="comment">     */</span></div><div class="line">    <span class="function">List&lt;InetSocketAddress&gt; <span class="title">getAllNimbusAddresses</span><span class="params">()</span></span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>一旦一个可用的nimbus出现，即调用addToLeaderLockQueue函数，将其加入可用nimbus名单。领导选举算法将从Queue中选出一个节点作为leader，若此时topology的代码、jar或者blob有丢失，则会从其他的正在运行的nimbus节点下载这些数据。</p>
<p>The first implementation will be Zookeeper based. If the zookeeper connection is lost/reset resulting in loss of lock or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the current status. The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not leader receives a request that only a leader can perform, it will throw a RunTimeException.</p>
<h2 id="Nimbus状态存储"><a href="#Nimbus状态存储" class="headerlink" title="Nimbus状态存储"></a>Nimbus状态存储</h2><p>（未完待续）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Storm中的分布式缓存&lt;br&gt;
    
    </summary>
    
      <category term="翻译" scheme="http://blog.tangkailin.cn/categories/%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="Distributed system" scheme="http://blog.tangkailin.cn/tags/Distributed-system/"/>
    
      <category term="Apache Storm" scheme="http://blog.tangkailin.cn/tags/Apache-Storm/"/>
    
  </entry>
  
  <entry>
    <title>关于Big-data的一点总结</title>
    <link href="http://blog.tangkailin.cn/2018/01/16/%E5%85%B3%E4%BA%8EBig-data%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%BB%E7%BB%93/"/>
    <id>http://blog.tangkailin.cn/2018/01/16/关于Big-data的一点总结/</id>
    <published>2018-01-16T15:48:59.000Z</published>
    <updated>2018-01-24T02:37:11.498Z</updated>
    
    <content type="html"><![CDATA[<p>关于Big-data的一点总结<br><a id="more"></a></p>
<h2 id="大数据知识架构"><a href="#大数据知识架构" class="headerlink" title="大数据知识架构"></a>大数据知识架构</h2><p>大数据是一个被炒烂了的概念。但大数据到目前为止也没有一个很清晰的定义出现。这种现象在计算机科学中其实十分常见，许多常见的概念如数据结构等，都是没有清晰定义的。这是因为作为一个工科类的学科，许多概念并不需要像基础学科那样，给出严谨的定义；此外，由于从业人群基数大，五花八门的概念层出不穷，这些概念难免会出现重叠之处，这也是计算机学科概念难以定义的一个主要原因。大数据作为一个新出的概念，也避免不了这样的命运。本文希望能够基于我个人的理解以及知识储备，谈一谈我对大数据的简单理解。如果有不妥或者不当之处，希望各位指点，多多交流。</p><br><p>大数据时代的到来，最重要的还是人们意识到了数据的重要性。正如我们以前没有意识到古董价值时，可以拿明代的碗来下面吃一样，只是原来我们没有发现数据的价值，没有搜集数据的意识而已，并非到了如今这个时代，才有这么多的数据出现。此外，大数据包含了哪些东西，如下图所示，我认为大数据理应包含三个部分。<br><br>- 计算能力。<br>- ​ 计算方法。<br>- 其他辅助工具。<br><br>首先是计算能力。应对海量数据的处理。有强大的计算能力支撑是很重要的一环。你1PB数据要跑一年才能跑出结果，再大的数据量也失去了意义。计算能力体现在快速的进行复杂计算（如Deep Learning）和快速的处理海量数据（如Distributed system）。要做到这两点并不容易，现行的一些方法大体思路分为两块：一块是并行化处理，包括并行计算和分布式计算等方式；另一块是针对特定应用使用特定的硬件设计来进行优化，包括高层次综合（如FPGA）以及TPU，寒武纪等。当然，前沿的还有革命性的量子计算，打破冯诺依曼体系的现代计算机结构的新型计算机。<br><br>其次是计算方法，这里的计算方法指的是如何利用海量数据。我认为大数据量的出现和机器学习以及深度学习的大火是相辅相成的。有了大量数据，大规模的ML，DL才能得以实现；而有了DL和ML，海量数据的价值才能被充分的挖掘（希望未来能有更先进的技术挖掘出更多的信息。）<br><br>最后，是大数据相关的一些技术。包括数据采集技术，如爬虫，传感器等等；数据检索技术，如ELK等；数据仓库技术，用于存储海量数据（多说一句，个人认为OLAP与OLTP唯一的区别在于，一个是动态的，一个是静态的。静态的OLTP只能存储数据，而动态的OLAP可以动态的对数据进行复杂的分析）；数据安全，如差分隐私等；以及大规模系统的运维、管理、组织、部署，如DevOps，容器技术等；都是为了方便技术人员或者从业人员更好的利用数据，更好的、更方便的使用系统而诞生的。这些技术并不一定都是用于大数据领域。<br><br></p>





<p><img src="http://ovjcsnet2.bkt.clouddn.com/BigData.png" alt="BigData"></p>
<p>暂时写这么多，想到了再补充。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于Big-data的一点总结&lt;br&gt;
    
    </summary>
    
      <category term="大数据学习总结" scheme="http://blog.tangkailin.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Big data" scheme="http://blog.tangkailin.cn/tags/Big-data/"/>
    
  </entry>
  
  <entry>
    <title>Serialize binary tree and Deserialize binary tree</title>
    <link href="http://blog.tangkailin.cn/2017/09/01/Serialize%20binary%20tree%20and%20Deserialize%20binary%20tree/"/>
    <id>http://blog.tangkailin.cn/2017/09/01/Serialize binary tree and Deserialize binary tree/</id>
    <published>2017-09-01T15:30:03.000Z</published>
    <updated>2018-01-24T02:35:21.640Z</updated>
    
    <content type="html"><![CDATA[<p>面试题一道。序列化与反序列化二叉树<br><a id="more"></a></p>
<p><a href="https://www.nowcoder.com/questionTerminal/cf7e25aa97c04cc1a68c8f040e71fb84" target="_blank" rel="external">牛客网真题链接</a></p>
<p>定义：</p>
<ol>
<li>序列化： 将数据结构或对象转换成二进制串的过程。</li>
<li>反序列化：将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。</li>
</ol>
<p>思路：</p>
<ol>
<li><p>序列化：将二叉树转化成一个字符串，每个节点用分隔符分开。由于二叉树的结构可以由其先序遍历和中序遍历结果来决定。一开始想到用二者的结合，后来觉得太麻烦了。不如直接就用一个，只要把整棵树用特殊字符将空值填充称完全二叉树就行了。有了这个思想，其实不管什么遍历，都可以实现序列化。</p>
</li>
<li><p>反序列化：模拟遍历。</p>
</li>
</ol>
<p>代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">package interview.leetcode;</div><div class="line"></div><div class="line">/**</div><div class="line"> * Created by 44931 on 2017/9/1.</div><div class="line"> */</div><div class="line">public class Solution2 &#123;</div><div class="line">    int index = -1;</div><div class="line"></div><div class="line">    public static class TreeNode &#123;</div><div class="line">        int val = 0;</div><div class="line">        TreeNode left = null;</div><div class="line">        TreeNode right = null;</div><div class="line"></div><div class="line">        public TreeNode(int val) &#123;</div><div class="line">            this.val = val;</div><div class="line"></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    String Serialize(TreeNode root, StringBuffer stringBuffer) &#123;</div><div class="line"></div><div class="line">        if (null == root) &#123;</div><div class="line">            return String.valueOf(stringBuffer.append(&quot;#,&quot;));</div><div class="line">        &#125;</div><div class="line">        stringBuffer.append(String.valueOf(root.val)+&quot;,&quot;);</div><div class="line">        Serialize(root.left, stringBuffer);</div><div class="line">        Serialize(root.right, stringBuffer);</div><div class="line">        return String.valueOf(stringBuffer);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    String Serialize(TreeNode root) &#123;</div><div class="line">        StringBuffer stringBuffer = new StringBuffer();</div><div class="line">        return Serialize(root, stringBuffer);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    TreeNode Deserialize(String str) &#123;</div><div class="line">        index++;</div><div class="line">        int len = str.length();</div><div class="line">        if(index &gt;= len)&#123;</div><div class="line">            return null;</div><div class="line">        &#125;</div><div class="line">        String[] strr = str.split(&quot;,&quot;);</div><div class="line">        TreeNode node = null;</div><div class="line">        if(!strr[index].equals(&quot;#&quot;))&#123;</div><div class="line">            node = new TreeNode(Integer.valueOf(strr[index]));</div><div class="line">            node.left = Deserialize(str);</div><div class="line">            node.right = Deserialize(str);</div><div class="line">        &#125;</div><div class="line">            return node;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    TreeNode Deserialize(String[] temp) &#123;</div><div class="line">        index++;</div><div class="line">        if (temp.length &lt; 1 || index &lt; 0 || temp.length &lt; index || temp[index].equals(&quot;#&quot;)) &#123;</div><div class="line">            return null;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        TreeNode root = new TreeNode(Integer.valueOf(temp[index]));</div><div class="line">        root.left = Deserialize(temp);</div><div class="line">        root.right = Deserialize(temp);</div><div class="line">        return root;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public static void frontSearch(TreeNode node) &#123;</div><div class="line">        if (node == null) &#123;</div><div class="line">            return;</div><div class="line">        &#125;</div><div class="line">        System.out.println(node.val);</div><div class="line">        frontSearch(node.left);</div><div class="line">        frontSearch(node.right);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public static void main(String[] args) &#123;</div><div class="line">        Solution2 solution2 = new Solution2();</div><div class="line">        TreeNode n1 = new TreeNode(1);</div><div class="line">        TreeNode n2 = new TreeNode(2);</div><div class="line">        TreeNode n3 = new TreeNode(3);</div><div class="line">        TreeNode n4 = new TreeNode(4);</div><div class="line">        TreeNode n5 = new TreeNode(5);</div><div class="line">        TreeNode n6 = new TreeNode(6);</div><div class="line">        TreeNode n7 = new TreeNode(7);</div><div class="line">        TreeNode n8 = new TreeNode(8);</div><div class="line">        TreeNode n9 = new TreeNode(9);</div><div class="line"></div><div class="line">        n1.left = n2;</div><div class="line">        n1.right = n3;</div><div class="line">        n2.left = n4;</div><div class="line">        n2.right = n5;</div><div class="line">        n3.left = n6;</div><div class="line">        n3.right = n7;</div><div class="line">        n4.left = n8;</div><div class="line">        n4.right = n9;</div><div class="line">        System.out.println(solution2.Serialize(n1));</div><div class="line">        frontSearch(solution2.Deserialize(solution2.Serialize(n1)));</div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;面试题一道。序列化与反序列化二叉树&lt;br&gt;
    
    </summary>
    
      <category term="算法" scheme="http://blog.tangkailin.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="Binary tree" scheme="http://blog.tangkailin.cn/tags/Binary-tree/"/>
    
  </entry>
  
  <entry>
    <title>leetcode-Letter Combinations of a Phone Number</title>
    <link href="http://blog.tangkailin.cn/2017/09/01/leetcode-Letter%20Combinations%20of%20a%20Phone%20Number/"/>
    <id>http://blog.tangkailin.cn/2017/09/01/leetcode-Letter Combinations of a Phone Number/</id>
    <published>2017-09-01T09:12:38.000Z</published>
    <updated>2018-01-24T02:35:10.850Z</updated>
    
    <content type="html"><![CDATA[<p>很有意思的一道题。<br><a id="more"></a><br>leetcode 原题：<br>Given a digit string, return all possible letter combinations that the number could represent.</p>
<p>A mapping of digit to letters (just like on the telephone buttons) is given below.</p>
<p>Input:Digit string “23”<br>Output: [“ad”, “ae”, “af”, “bd”, “be”, “bf”, “cd”, “ce”, “cf”].<br>Note:<br>Although the above answer is in lexicographical order, your answer could be in any order you want.</p>
<p>给定一个数字字符串，每个数字和字母的映射参考功能机的键盘。求有多少种不同的字母组合。</p>
<p>这个题其实很简单。每个数字对应3-4个字母，有n个数字，输出所有可能的组合。数字顺序不能调换，是一个组合问题而不是排列问题。很直观的就想到使用回溯，递归的方法。提取第i个数字对应的字母(使用字典咯)，针对每个字母递归进入第i+1个数字直到i=n,返回当前生成的字符串。</p>
<p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">class Solution &#123;</div><div class="line">     public List&lt;String&gt; letterCombinations(String digits) &#123;</div><div class="line">        ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();</div><div class="line">        if (digits.isEmpty()) &#123;</div><div class="line">            return list;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String[] dict = new String[]&#123;&quot; &quot;, &quot;&quot;, &quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;, &quot;jkl&quot;, &quot;mno&quot;, &quot;pqrs&quot;, &quot;tuv&quot;, &quot;wxyz&quot;&#125;;</div><div class="line">        letterCombinationsRecursion(digits, dict, 0, &quot;&quot;, list);</div><div class="line">        return list;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    private void letterCombinationsRecursion(String digits, String[] dict, int level, String out, ArrayList&lt;String&gt; list) &#123;</div><div class="line">        if (level == digits.length()) &#123;</div><div class="line">            list.add(out);</div><div class="line">        &#125; else &#123;</div><div class="line">            String str = dict[digits.charAt(level) - &apos;0&apos;];</div><div class="line">            for (int i=0; i&lt;str.length(); i++) &#123;</div><div class="line">                out += str.charAt(i);</div><div class="line">                letterCombinationsRecursion(digits, dict, level+1, out, list);</div><div class="line">                out = out.substring(0,out.length()-1);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>思考：看到这类限定性很强，或者枚举空间很小的题，总是会松懈，这样不行，做事要严谨一点。不要什么都想着用字典啊之类的暴力的方法，多想一些漂亮的数学上的解法。<br>更多解法：<a href="http://www.cnblogs.com/grandyang/p/4452220.html" target="_blank" rel="external">here</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很有意思的一道题。&lt;br&gt;
    
    </summary>
    
      <category term="算法" scheme="http://blog.tangkailin.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="String" scheme="http://blog.tangkailin.cn/tags/String/"/>
    
      <category term="leetcode" scheme="http://blog.tangkailin.cn/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>Storm, Flink和spark Streaming中的backpressure机制比较</title>
    <link href="http://blog.tangkailin.cn/2017/08/30/Storm,%20Flink%E5%92%8Cspark%20Streaming%E4%B8%AD%E7%9A%84backpressure%E6%9C%BA%E5%88%B6%E6%AF%94%E8%BE%83/"/>
    <id>http://blog.tangkailin.cn/2017/08/30/Storm, Flink和spark Streaming中的backpressure机制比较/</id>
    <published>2017-08-30T15:22:56.000Z</published>
    <updated>2018-01-26T14:43:28.311Z</updated>
    
    <content type="html"><![CDATA[<p> Storm, Spark streaming, Flink中的backpressure机制比较。<br><a id="more"></a></p>
<p><br>backpressure是分布式流处理系统中一种有效的流量控制手段，为了防止负载过大，计算瓶颈等对整个系统带来的影响。backpressure基本上是每个流处理系统都应该考虑的一种流量控制手段。流量控制的方法有很多种，有兴趣的可以去看看网络中是怎么控制流量的。分布式流其实和网络流的本质是一样的。<br></p><br><p><br>本文先分别讨论各个系统的backpressure机制，然后再总结三者的异同。<br></p>

<h2 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h2><p>storm的backpressure功能流程如下：</p><br><p>在worker中会有一个backpressure线程，实时的监控executor的receive queue或者worker的transfer queue的状态，一旦发现某一queue中的数据量超过一个阈值（由 <strong>backpressure.disruptor.high.watermark</strong> 配置，默认为0.9），即触发backpressure，此时backpressure线程会将这当前topology的信息写入zookeeper，watcher检测到zookeeper中的数据变化，则立马通知所有worker进入backpressure状态，上游spout停止发送数据,直到queue中的数据量低于一个阈值（由 <strong>backpressure.disruptor.low.watermark</strong> 配置，默认为0.4）。所有worker退出backpressure状态，spout正常发送数据。</p><br>executor.clj中代码段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">(if (and (not (.isFull transfer-queue))</div><div class="line">                     (not throttle-on)</div><div class="line">                     (not reached-max-spout-pending))</div><div class="line">                 (fast-list-iter [^ISpout spout spouts] (.nextTuple spout))))</div></pre></td></tr></table></figure><br><br><p>不了解disruptor queue在storm中的使用的可以参考：<a href="http://www.michael-noll.com/blog/2012/10/16/understanding-the-parallelism-of-a-storm-topology/" target="_blank" rel="external">Understanding the Parallelism of a Storm Topology</a><br></p><br><p><br>storm中的backpressure还是相当暴力的。这样子其实整个系统都处在一个不稳定的状态，堵住了马上就不发送任何数据进来，直到疏通了，开始发送，然后慢慢堵住，直到触发backpressure。<br></p>

<p>storm backpressure具体内容参考：<a href="https://github.com/apache/storm/pull/700" target="_blank" rel="external">storm-886</a></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://ovjcsnet2.bkt.clouddn.com/stormbackpressure.jpg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><p>Flink是新一代的流处理系统，其backpressure的实现方式相比于storm有很大的区别。其不再借助zookeeper或者其他的外部组件来实现backpressure。而是在其内部数据传输时用一种类似阻塞队列的方式很合理，很漂亮的实现了这一功能。</p><br><p>具体流程已经有人写了很好的<a href="http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/" target="_blank" rel="external">博文</a>了,我就不再多次一举了。</p>

<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p>待研究。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Storm, Spark streaming, Flink中的backpressure机制比较。&lt;br&gt;
    
    </summary>
    
      <category term="Storm知识" scheme="http://blog.tangkailin.cn/categories/Storm%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Distributed system" scheme="http://blog.tangkailin.cn/tags/Distributed-system/"/>
    
      <category term="Apache Storm" scheme="http://blog.tangkailin.cn/tags/Apache-Storm/"/>
    
      <category term="Spark" scheme="http://blog.tangkailin.cn/tags/Spark/"/>
    
      <category term="Flink" scheme="http://blog.tangkailin.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>leetcode-Longest Substring Without Repeating Characters</title>
    <link href="http://blog.tangkailin.cn/2017/08/23/leetcode-Longest%20Substring%20Without%20Repeating%20Characters/"/>
    <id>http://blog.tangkailin.cn/2017/08/23/leetcode-Longest Substring Without Repeating Characters/</id>
    <published>2017-08-23T14:53:38.000Z</published>
    <updated>2018-01-24T02:37:11.497Z</updated>
    
    <content type="html"><![CDATA[<p>寻找最长无重复子串的最优解法<br><a id="more"></a><br>leetcode 原题：<br>Given a string, find the length of the longest substring without repeating characters.</p>
<p>Examples:</p>
<p>Given “abcabcbb”, the answer is “abc”, which the length is 3.</p>
<p>Given “bbbbb”, the answer is “b”, with the length of 1.</p>
<p>Given “pwwkew”, the answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring.</p>
<p>网上有很多不同类型的解法，这里不做一一介绍，提供一个时间复杂度O(n)，空间复杂度O(1)的解法如下：</p>
<p>思路：</p>
<ol>
<li><p>该问题的通常解法是遍历每个字符起始的子串，找出结果。在计算过程中使用hash存储中间结果。通过遍历的方式时间复杂度为O(n^2)。</p>
</li>
<li><p>可以考虑使用DP，因为对于字符串中的一个字符而言，如果其与 <em>其前面字符结尾的最长不重复子串</em> 中的字符没有重复的话。那么就可以将其加入到最长不重复子串中，则以当前字符结尾的子串长度为其前面找到的子串长度+1。若有重复，如果有重复，且重复位置在上一个最长子串起始位置之后，那么就与该起始位置之后的稍短的子串构成新的子串或者单独成一个新子串。dp表中记录以每个字符结尾的最长不重复子串。<br>例如：有字符串abccd,有dp[0]=1,dp[1]=2,dp[2]=3。那么当找到第二个c时，此时以第一个c结尾的最长不重复子串长度为dp[2]=3（abc），发现有重复，则修改：dp[3]=1。找到d时，与前面的不重复子串没有相同字符，故dp[4]=2。<br>这样的dp算法的时间复杂度为O(n^2)。因为每次我们都需要“回头”去寻找重复元素的位置。</p>
</li>
<li><p>怎么样才能不回头找呢？用hash咯。可以用hash记录元素是否出现过，key为元素值，value为元素上一次出现的位置。同时可以发现，其实我们不需要维护一张DP表，因为我们只需要记住以当前元素的前一个元素结尾的最长不重复子串的长度即可。所以可以进一步优化空间使用从O(n) -&gt; O(1)。</p>
</li>
</ol>
<p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line"></div><div class="line">    public int lengthOfLongestSubstring(String s) &#123;</div><div class="line">        if (s == null || s.length() &lt;= 0)</div><div class="line">            return 0;</div><div class="line">        int maxLen = 1;</div><div class="line">        int lastIndex = 0;</div><div class="line">        int current = 1;</div><div class="line">        HashMap&lt;Character,Integer&gt; map = new HashMap&lt;&gt;();</div><div class="line">        map.put(s.charAt(0),0);</div><div class="line">        for (int i=1; i&lt;s.length(); i++) &#123;</div><div class="line">            if (map.get(s.charAt(i)) == null) &#123;</div><div class="line">                map.put(s.charAt(i), i);</div><div class="line">                current++;</div><div class="line">            &#125; else if (lastIndex &lt;= map.get(s.charAt(i))) &#123;</div><div class="line">                current = i - map.get(s.charAt(i));</div><div class="line">                lastIndex = map.get(s.charAt(i))+1;</div><div class="line">                map.put(s.charAt(i), i);</div><div class="line">            &#125; else &#123;</div><div class="line">                current++;</div><div class="line">                map.put(s.charAt(i), i);</div><div class="line">            &#125;</div><div class="line">            if (current &gt; maxLen) &#123;</div><div class="line">                maxLen = current;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        return maxLen;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>参考：<br><a href="http://xfhnever.com/2014/10/30/algorithm-lnrs/" target="_blank" rel="external">http://xfhnever.com/2014/10/30/algorithm-lnrs/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;寻找最长无重复子串的最优解法&lt;br&gt;
    
    </summary>
    
      <category term="算法" scheme="http://blog.tangkailin.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="String" scheme="http://blog.tangkailin.cn/tags/String/"/>
    
      <category term="leetcode" scheme="http://blog.tangkailin.cn/tags/leetcode/"/>
    
  </entry>
  
</feed>
