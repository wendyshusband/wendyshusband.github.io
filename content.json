[{"title":"Docker相关的一些关键概念","date":"2018-02-02T15:02:31.000Z","path":"2018/02/02/Docker相关的一些关键概念/","text":"关于Docker中的一些概念的个人理解。 之前做了几个docker相关的项目，一直没有好好的总结过。现在对之前的知识进行一下简单的梳理。总结一下docker相关的一些关键概念。水平有限，不当之处欢迎指正。 Docker容器和虚拟机的区别容器和虚拟机到底有什么区别，看了很多文章、很多资料。发现没几个能够讲得清楚的。还是要自己在实践中去摸索。所以说实践是检验真理的唯一标准，这个命题在科学领域还是很必要去贯彻的。 讲过大道理之后，让我们来真正的探讨这个问题。 虚拟机是什么？ 百度百科这样定义——“虚拟机（Virtual Machine）指通过软件模拟的具有完整硬件系统功能的、运行在一个完全隔离环境中的完整计算机系统。”。wikipedia这样定义——“In computing, a virtual machine (VM) is an emulation of a computer system. Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.”。从定义中我们可以看出虚拟机是一个完全仿真的计算机系统。随着云计算技术的兴起，虚拟机的应用迎来了一个前所未有的高潮。 Docker又是什么呢？Docker官方给出的定义是：”Run the application anywhere: Docker containers wrap a piece of software in a complete filesystem that contains everything needed to run. This guarantees that the software will always run the same, regardless of its environment.”。说白了，Docker的目的在于提高应用程序的可移植性和兼容性。其手段就是将程序封装到容器(container)中。这些容器可以部署在任何装有docker的机器之上。 从定义上来看，其实这两个是完全不同的技术，适用场景也不尽相同。为什么会拿两者来进行比较呢？原因在于传统的认识一直存在误区。这就是认为vm和docker都是将放在一个隔离环境里跑，所以他们是可以进行比较的。其实在我看来这个比较是不太恰当的。隔离性是vm的核心特色之一，而并非docker的核心，docker运行的目的是为了 Run the application anywhere 。vm使用hypervisor等技术，实现了资源隔离，这里的隔离目的在于对营造一个独立的运行环境，这个环境在云计算或者安全领域都有着广泛的应用。而docker可以理解成只是将一个application和其必须的一些dependencies放到容器当中去，利用namespace和control group等技术实现隔离运行，这里的隔离运行目的在于使得应用可移植。这是区别一。 区别二：vm是营造了一个虚拟的计算机环境，因此一台vm需要包含一台计算机的所有的、完备的功能，vm是一个庞大的体系。而一个docker容器，是解决某个特殊问题的，只包含了解决这一问题的依赖包，容器只是计算机中运行的一个进程（process）。 以上。 Docker底层的三个关机技术namespacecontrol group分层文件系统未完待续","comments":true,"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.tangkailin.cn/tags/Docker/"}]},{"title":"storm上指定slot分配operator的scheduler","date":"2018-02-01T15:41:59.000Z","path":"2018/02/01/storm上指定slot分配operator的scheduler/","text":"storm上实现分配operator到指定slot的scheduler motivation在storm里，需要对多个不同的operator进行调度和资源分配。默认的调度器为EvenScheduler，也有一些其他的调度器如：Isolation Schduler, ResourceAwareScheduler，MultitenantScheduler等，具体参考官方文档。另外，简要介绍在这里。在我们的实验中，需要对网络功能进行一些测试和研究。因此希望能够实现slot和operator的绑定。而不是node和operator的绑定。在storm源码里没有这样的scheduler。github上，在这里有一个将指定operator分配到指定node的scheduler，基于linyiqun的工作，我们稍微进行了一些改进，实现了一个配operator到指定slot的scheduler –DirectToSlotScheduler，具体代码在这里 design1. 根据用户指定的slot和operator的关系来分配资源。包括两个配置项： - assigned_flag : 绑定到topology，若为1 ，则该topology使用DirectToSlotScheduler。否则使用EvenScheduler - design_map： 绑定到topology，为一个map。key为operator名称，value为一个字符串–“host1:port1;host2:port2;…;hostN:portN”。slot之间用；分隔。2. operator包含的多个executor会均匀的分布到指定的slot上。假如指定的slot数量大于executor数，则报错。3. 对于系统默认的bolt，如__ack，则是随机分配slot。 test测试用例","comments":true,"tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Apache Storm","slug":"Apache-Storm","permalink":"http://blog.tangkailin.cn/tags/Apache-Storm/"},{"name":"resource scheduling","slug":"resource-scheduling","permalink":"http://blog.tangkailin.cn/tags/resource-scheduling/"}]},{"title":"推荐站点","date":"2018-01-28T15:36:59.000Z","path":"2018/01/28/推荐站点/","text":"一些站点推荐。 技术博客Linux及其他：觅风SuperShen PHP相关：tc294682552的博客","comments":true,"tags":[{"name":"推荐","slug":"推荐","permalink":"http://blog.tangkailin.cn/tags/推荐/"}]},{"title":"有些人是注定没有梦想的","date":"2018-01-27T12:36:59.000Z","path":"2018/01/27/有些人是注定没有梦想的/","text":"观《神秘巨星》。 有些人是注定没有梦想的，看到这句话，热泪盈眶。他们的梦想消磨在年复一年的承担责任；他们的梦想消磨在日复一日的照顾我们的日子里。有些道理，我们好像知道，却永远不会去重视；有些东西，我们心里知道，却永远不敢去面对和承认。面对我们的父母，我们心里知道要关心他们；知道要多陪伴他们；我们想到为他们买礼物；带他们做体检…… 却总会在某些时候去选择性的忽略他们。这是必须要面对的事实，只是你心里不敢承认。这或许是人性，人们总是同情弱者，崇拜强者。小时候我们崇拜父母，觉得他们是最强的依靠；长大后我们讨厌父母，觉得他们束缚了我们的翅膀；最终我们成为父母，才会知道父亲、母亲这两个称谓的分量。 主人公所处的家庭似乎是个无解的局，家暴而独裁的父亲，懦弱的母亲，还小的弟弟，老迈的奶奶。这一切，就这么过着。父亲因为一串项链、一顿忘记放盐的饭就会暴打母亲，奶奶永远默不作声，弟弟还小，只有主人公和母亲来承受这一切。主人公希望和母亲逃跑，但母亲却他们却无法离开父亲，离开父亲似乎会失去生活的来源和支柱。但不离开父亲就要承受压抑的人生。这似乎是很多“凑活着过”的家庭的状态，这是一个死局，你的一生很可能就在这个局里面死去。母亲其实清楚的认识到了这个点，但是为了子女，她甘愿死在这个局里。这一切都由奶奶最后的一番话来点明，这段话也为最后母亲的爆发埋下了伏笔。整部电影都处于一种压制的状态，死局一直存在着，但也不乏一些注入夏提克和胶带粘电脑这样的欢快的桥段，其间分寸拿捏得很好。 最后母亲的爆发，打破了死局。其实死局之所以成为死局，是因为局中的人。其实只要一点的努力，所有的局都是活局。母亲为了主人公入局，也为了主人公而出局。母亲注定没有梦想，她却成就了主人公的飞翔。热泪盈眶。 父亲却还在局中，父亲每天工作17小时。兢兢业业的工程师，希望能够获得更好的社会地位。他逃不开社会这个局。社会给他压力，令他麻木。他活在自己的传统中，树立着虚无的威信。这些暴力手段才能让他感觉自己还活着。他陶醉于自己取得的可怜的成绩，却从不质疑自己哪方面做得不够。他嫌弃母亲没文化，自命清高，却比谁都要庸俗。这其实是这个社会大多数人的常态，我看到过太多这样的人。他们是这个社会的牺牲品，他们接收教育，用自己的知识来服务这个社会。通过出卖自己的服务，换取可怜的报酬。他们没有梦想，他们的所谓梦想是被社会压抑的畸形梦想。他们也是可怜人啊。 阿米尔汗的电影，其实讲述的都是很简单的一个故事，反映一个好像大家都明白却往往忽略了的道理。从印度这个包含了很多陈旧思想的国家的角度来说，实在是难能可贵。这部电影或许因为故事情节相对压抑的原因，感觉不如以前他的电影看着那么爽快 ，那么酣畅。但是，压抑的才是真实的人生啊。人生啊，就是由无数的遗憾组成的。 为什么我们能站得更高，看得更远。因为我们站在巨人的肩膀上！You raise me up.","comments":true,"tags":[{"name":"影评","slug":"影评","permalink":"http://blog.tangkailin.cn/tags/影评/"}]},{"title":"Running-mapreduce-job-into-Docker-container","date":"2018-01-23T15:02:58.000Z","path":"2018/01/23/Running-mapreduce-job-into-Docker-container/","text":"Welcome to contribute to our documentation on github. The Hadoop 2.6 release contains a new feature that allows to launch Docker container directly as YARN container, called the DCE (Docker container executor). Running a Hadoop job is not an easy thing, User need to solve a lot of problem, such as complex problem of dependency. Using DCE let the developers package their applications and all of dependencies into a Docker container in order to provide a consistent environment for execution and also provides isolation from other applications installed on host. Because of the official reference document is relatively simple, and ignored a lot of details, this tutorial will describe the detail of DCE configuration and some problems needing attention. PrerequisiteYour Hadoop version must be 2.7.i at minimum. The distro and version of Linux in your Docker image can be quite different from that of your Nodemanager. However, if you are using the MapReduce framework, then your Docker image will need to be configured for running Hadoop. Obviously, Java is in need, and the following environment variables must be defined in the image: JAVA_HOME, HADOOP_COMMON_PATH, HADOOP_HDFS_HOME, HADOOP_MAPRED_HOME, HADOOP_YARN_HOME, and HADOOP_CONF_DIR.Before running the DCE, please make sure that your host has installed Hadoop and Docker. Experimental EnvironmentYou will have 3 machines running Ubuntu Server 14.04, each of which will be running a Docker daemon and Hadoop 2.7.3 inside. Besides we are using sequenceiq/hadoop-docker2.4.1 image that developed by sequenceiq. This image already contains all of required environmental dependencies. Of course, you can also using your own Docker image Experimental Procedure##Step 1：Pull the Docker image You can use the following command to pull image: docker pull sequenceiq/hadoop-docker:2.4.1 and, you can type the following command to inspect the running environment into the Docker container: docker run -it sequenceiq/hadoop-docker:2.7.1 /etc/bootstrap.sh -bash If nothing has gone wrong, you should now find the Hadoop directory into /usr/local that inside the container. Step 2：Configure the yarn-site.xml on the hostYou need to add the following contents into yarn-site.xml: \\ \\yarn.nodemanager.docker-container-executor.exec-name\\ \\/usr/bin/docker\\\\\\ \\yarn.nodemanager.container-executor.class\\ \\org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor\\\\ The first configuration items yarn.nodemanager.docker-container-executor.exec-name is configure the path of the Docker executable file. And the second items yarn.nodemanager.docker-container-executor.class is configure the DockerContainerExecutor as container executor rather than the default container executor into YARN. Step 3：Restart the YARN and HDFS on the host ./sbin/stop-all.sh./sbin/start-all.sh Step 4：Submit a MapReduce jobHere, we are using the hadoop-mapreduce-examples-2.7.3.jar π calculation.Type following command: bin/Hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi-D mapreduce.map.env=”yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.4.1”-D mapreduce.reduce.env=”yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.4.1”-D yarn.app.mapreduce.am.env=”yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.4.1” 5 10 Currently you cannot configure any of the Docker settings with the job configuration. You can provide Mapper, Reducer, and ApplicationMaster environment overrides for the docker images, using the following 3 JVM properties respectively(only for MR jobs):-mapreduce.map.env: You can override the mapper’s image by passing yarn.nodemanager.docker-container-executor.image-name=your_image_name to this JVM property.-mapreduce.reduce.env: You can override the reducer’s image by passing yarn.nodemanager.docker-container-executor.image-name=your_image_name to this JVM property.-yarn.app.mapreduce.am.env: You can override the ApplicationMaster’s image by passing yarn.nodemanager.docker-container-executor.image-name=your_image_name to this JVM property. If noting has gone wrong, you can docker ps command to affirm Docker container is running. And you can find that the name of Docker container is same as the default YARN container name that show on the terminal. Need to be aware of problem in the experiment Don`t use Hadoop 3.0.0 or higher version, There will be some puzzling error. Error: No such image or containerThis error may be because of the Docker image version do not match the Hadoop version on the host. And you can refer to log file of Nodemanager. Diagnostics: Container image must not be nullTwo reasons led to this error:A. The Hadoop running no hosts do not support using DCE as YARN container.B. The configuration of YARN is wrong or when you submit job, you forget configure for DCE.Note. DCE and default YARN container can’t be use at same time in cluster. Error: No such image , container or taskThis error is not that simple. You can refer to error log file (stderr) of MapReduce job. If you find message about Error: Could not find or load main class org.apache.-hadoop.mapreduce.v2.app.MRAppMaster. You can add the library path that is inside the Docker container to the classpath of MapReduce job in mapred-site.xml. If you find message about Exception in thread “main” java.lang.NoClassDefFoundError:-org/apache/hadoop/service/CompositeService. You need also add the library path that is inside the Docker container to the classpath of YARN in yarn -site.xml. The MR job is stuck in the accepted.You can refer to log file of Nodemanager. This error maybe result from resource that schedule by YARN is not enough, then you need to increase the number of resource in yarn-site.xml. May also result from the Docker container can’t get the PID, at this time, please read this article seriously.","comments":true,"tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Map Reduce","slug":"Map-Reduce","permalink":"http://blog.tangkailin.cn/tags/Map-Reduce/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.tangkailin.cn/tags/Docker/"}]},{"title":"Deploy-storm-on-docker","date":"2018-01-23T14:59:09.000Z","path":"2018/01/23/deploy-storm-on-docker/","text":"Welcome to contribute to our documentation on github. This guide reference The joy of deploying Apache Storm on Docker Swarm in Baqend Tech.And we describe some of the detail and principles about every step in this tutorial.If you have already have a good use of Linux,Docker swarm and Apache storm,you can use script given in the article above for rapid deployment. This tutorial is targeted at beginner novice.We will manual configuration on each machine and you will be more clear understand the principle in the process of deployment . So let’s begin : Overall Architecture You’ll have 3 machines running Ubuntu Server 14.04, each of which will be running a Docker daemon with several containers inside. As shown in figure above ,we use ubuntu1 as manager of Docker swarm. The Nimbus and UI containers will be spawned on the manager node (Ubuntu 1).Beside ,remember to open port 8080 for UI container.When swarm is in place ,you’ll create an overlay network (stormnet) to enable communication between Docker container hosted on the different swarm nodes. Finally, you will set up a full-fledged storm cluster that uses the existing zookeeper ensemble for coordination and stormnet for inter-node communication. This involves discovery service of Docker swarm ,you can read here.We are using hostnames dmir1,dmir2 and dmir3 for the three Ubuntu machines. Using alias rather than an IP address can have better fault tolerance. At the time of the actual deployment ,remember to replace your own domain name. Three zookeeper severs are deployed here, of course,using one zookeeper is also possible. Install DockerUsing your favorite way to connect three machines and install Docker on each machine. You can reference here to install Docker. After the installation is complete,we should test to ensure that the installation is successful. Create /etc/init.shCreate the file used to configure the Docker swarm worker. In the terminal ,type : sudo touch /etc/init.shsudo vim /etc/init.sh and then paste the following and save: #!/bin/bash# first script argument: the servers in the ZooKeeper ensemble:ZOOKEEPER_SERVERS=$1# second script argument: the role of this node:# (“manager” for the Swarm manager node; leave empty else) ROLE=$2 # the IPaddress of this machine:PRIVATE_IP=$(/sbin/ifconfig eth0 | grep ‘inet addr:’ | cut -d: -f2 | awk ‘{ print &gt;$1}’)# define label for the manager node: if [[ $ROLE == “manager” ]];then LABELS=”–label server=manager”;else LABELS=””;fi# define default options for Docker Swarm:echo “DOCKER_OPTS=\\”-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock –cluster&gt;-advertise eth0:2375 $LABELS –cluster-store zk://$ZOOKEEPER_SERVERS\\”” | sudotee /etc/default/docker# restart the service to apply new options:sudo service docker restartecho “let’s wait a little…”sleep 10# make this machine join the Docker Swarm cluster:docker run -d –restart=always swarm join –advertise=$PRIVATE_IP:2375zk://$ZOOKEEPER_SERVERS In this shell file, we configure some detail for start a swarm worker. The first step , we defined three zookeeper server node ,through the ZOOKEEPER_SERVERS variable. The second step, we parse /sbin/ifconfig eth0 to get the IP address of the machine($PRIVATE_IP). The third step, we mark the label “manager ” for swarm manager node. The fourth step, modify /ect/default/docker, including some content such as docker daemon listening port 2375,which can refer to the configuration of the docker on the https://docs.docker.com/. The fifth step, restart Docker service. The last step, docker run a swarm worker container ,and use zookeeper as the service discovery. Create swarm workerIn the terminal of Ubuntu1 ,type : /bin/bash /etc/init.sh dmir1,dmir2,dmir3 manager In the terminal of Ubuntu2 and Ubuntu3, type : /bin/bash /etc/init.sh dmir1,dmir2,dmir3 set up your DNS in such a way that the first hostname in the list (dmir1) points towards the manager on Ubuntu 1 and the other two hostnames (dmir2 and dmir3) point towards the other two machines, i.e. Ubuntu 2 and Ubuntu 3.Configure your security settings to allow connections between the machines on ports 2181, 2888, 3888 (zookeeper), 2375 (Docker Swarm) and 6627 (Storm, remote topology deployment).If nothing has gone wrong, you should now have three Ubuntu servers, each running a Docker daemon. Ubuntu 1 should be reachable via dmir1 in your private network. Now, we can only configuration on Ubuntu1, it is going to be the only machine you will talk to from this point on. Start Docker swarm cluster and zookeeper serverUsing docker ps to perform a quick heal check. If Docker is installed correctly, the terminal will show a list of the running Docker container (exactly 1 for swarm and nothing else ) You are now good to launch one zookeeper node on every machine like this: docker -H tcp://dmir1:2375 run -d –restart=always -p 2181:2181 -p 2888:2888 -p3888:3888 -v /var/lib/zookeeper:/var/lib/zookeeper -v /var/log/zookeeper:/var/log/zookeeper –name zk1 baqend/zookeeper dmir1,dmir2,dmir3 1docker -H tcp://dmir2:2375 run -d –restart=always -p 2181:2181 -p 2888:2888 -p 3888:3888 -v /var/lib/zookeeper:/var/lib/zookeeper -v /var/log/zookeeper:/var/log/zookeeper –name zk2 baqend/zookeeper dmir1,dmir2,dmir3 2docker -H tcp://dmir3:2375 run -d –restart=always -p 2181:2181 -p 2888:2888 -p 3888:3888 -v /var/lib/zookeeper:/var/lib/zookeeper -v /var/log/zookeeper:/var/log/zookeeper –name zk3 baqend/zookeeper dmir1,dmir2,dmir3 3 Here we using baqend/zookeeper image on the docker hub.By specifying the -H … argument, we are able to launch the zookeeper containers on the different host machines. The -p commands expose the ports required by zookeeper per default. The two -v commands provide persistence in case of container failure by mapping the directories the zookeeper container uses to the corresponding host directories. The comma-separated list of hostnames tells zookeeper what servers are in the ensemble. This is the same for every node in the ensemble. The only variable is the zookeeper ID (second argument), because it is unique for every container. To check zookeeper health, you can do the following: docker -H tcp://dmir1:2375 exec -it zk1 bin/zkServer.sh status &amp;&amp; docker -Htcp://dmir2:2375 exec -it zk2 bin/zkServer.sh status &amp;&amp; docker -Htcp://dmir3:2375 exec -it zk3 bin/zkServer.sh status Here related to Docker exec command and the zookeeper service status command bin/zkServer.sh status. If your cluster is healthy, every node will report whether it is the leader or one of the followers. If something goes wrong,likely due to $PRIVATE_IP can’t be parsed, then we recommended that removed the last step in the init.sh and manually docker run a swarm worker on each node. docker run -d –restart=always swarm join –advertise=$PRIVATE_IP:2375zk://$ZOOKEEPER_SERVERS After you start the zookeeper service ,you need to start the swarm manager: docker run -d –restart=always –label role=manager -p 2376:2375 swarmmanage zk://dmir1,dmir2,dmir3 Now the Swarm cluster is running. However, we still have to tell the Docker client about it. So finally, you only have to make sure that all future docker run statements are directed to the Swarm manager container (which will do the scheduling) and not against the local Docker daemon: cat &lt;&lt; EOF | tee -a ~/.bash_profile# this node is the master and therefore should be able to talk to the Swarmcluster:export DOCKER_HOST=tcp://127.0.0.1:2376EOF This will do it for the current session and also make sure it will be done again when we log into the machine next time.Now everything should be up and running. Type in docker info to check cluster status on the manager node. You should see 3 running workers similar to this: The important part is the line with Status: Healthy for each node. If you observe something like Status: Pending or if not all nodes show up, even though you are not experiencing any errors elsewhere, try restarting the manager container like so: docker restart $(docker ps -a –no-trunc –filter “label=role=manager) Setup the storm clusterFirst, create the overlay network stormnet. This network is guaranteed the mapping relationship between container and physical machine.you can reference here. docker network create –driver overlay stormnetdocker network ls This creates a network and detect if created successfully. If something goes wrong,maybe your swarm cluster is not running. It means that you should refer to the previous tutorial and guarantee every step is carefully finished.Now, let’s start storm components, including UI, nimbus and supervisor. First, start the UI: docker run -d –label cluster=storm –label role=ui -e constraint:server==manager -e STORM_ZOOKEEPER_SERVERS=dmir1,dmir2,dmir3 –net stormnet –restart=always –name ui -p 8080:8080 baqend/storm ui and the nimbus : docker run -d –label cluster=storm –label role=nimbus -econstraint:server==manager -e STORM_ZOOKEEPER_SERVERS=dmir1,dmir2,dmir3 –netstormnet –restart=always –name nimbus -p 6627:6627 baqend/storm nimbus For specific configuration can refer to baqend/storm. The -p commands expose the ports required by UI container and nimbus container default. The -net command specified the container under the stormnet, in order to communicate with the container on other physical machine.Note that nimbus parameter are added after baqend/storm, when we start nimbus container. This parameter is mark the nimbus container as the main container. This involves some configuration of baqend/storm, you can view here.To make sure that these are running on the manager node, we specified a constraint : constraint : server==manager. You can now access the Storm UI as though it would be running on the manager node, However, there are no supervisors running, yet. Finally, start the supervisor, you can start supervisor on any machine. docker run -d –label cluster=storm –label role=supervisor -eaffinity:role!=supervisor -e STORM_ZOOKEEPER_SERVERS=dmir1,dmir2,dmir3 –netstormnet –restart=always baqend/storm supervisor -c supervisor.slots.ports=[6700] Since we do not care where exactly the individual supervisors are running, we did not specify any constraints or container names here. However, in order to prevent two supervisors from being hosted on one machine, we did specify a label affinity : affinity : role!=supervisor. If you need more supervisor containers, you’ll have to add additional Swarm worker nodes (Ubuntu 4, Ubuntu 5, …). Besides, the -c supervisor.slots.ports=[6700] provided one port 6700 for supervisor. Of course you can provide any number of port for supervisor. But we recommend that you use only one port, because the container is essentially a process on physical machine , no matter how much the port resources are the same.Have a look at the Storm UI and make sure that you have supervisors running. Topology DeploymentDeploying a topology can now be done from any server that has a Docker daemon running and is in the same network as the manager machine. The following command assumes that your topology fatjar is a file called topology.jar in your current working directory: docker -H tcp://127.0.0.1:2375 run -it –rm -v $(readlink -m topology.jar):/topology.jar –net stormnet baqend/storm jar /topology.jar main.class arg1 arg2 This command will spawn a Docker container, deploy the topology and then remove the container. You should provide the -H tcp://127.0.0.1:2375 argument to make sure the container is started on the machine you are currently working on; if you left the scheduling to Docker Swarm, the deployment might fail because the spawning host does not necessarily have the topology file.By the way, we use readlink -m topology.jar which produces an absolute path fortopology.jar, because relative paths are not supported. You can also provide an absolute path directly, though.Last but most important , –net stormnet is necessary, because you spawn a new container for deploy the topology, so this new container must under the stormnet ,otherwise it will return a unknownhost error. Killing a TopologyKilling the topology can either be done via the Storm web UI interactively or, assuming the running topology is called runningTopology, like this: docker run -it –rm –net stormnet baqend/storm kill runningTopology The host argument -H … is not required here, because the statement stands on its own and has no file dependencies. Topology RebalanceTopology rebalance can either be done via the Storm web UI interactively or, assuming the running topology is called runningTopology, like this: docker run -it –rm –net stormnet baqend/storm rebalance runningTopology -n arg or: docker exec -it nimbus storm rebalance runningTopology -n arg Of course , we need enough slot to support this operation. Shutting down the storm clusterSince every Storm-related container is labelled with cluster=storm, you can kill all of them with the following statement: docker rm -f $(docker ps -a –no-trunc –filter “label=cluster=storm” Finally, we finish our work. we stepped over how to configure Docker Swarm for TLS. If you are planning to use Docker Swarm in a business-critical application, you should definitely put some effort into this aspect of deployment. You can use our script for repid depolyment","comments":true,"tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.tangkailin.cn/tags/Docker/"},{"name":"Apache Storm","slug":"Apache-Storm","permalink":"http://blog.tangkailin.cn/tags/Apache-Storm/"}]},{"title":"Storm中的分布式缓存","date":"2018-01-20T02:50:56.000Z","path":"2018/01/20/Storm中的分布式缓存/","text":"Storm中的分布式缓存 原文地址：http://storm.apache.org/releases/1.1.1/distcache-blobstore.html 翻译水平有限，欢迎各位指正。 Storm 分布式缓存API Storm中的分布式缓存的主要用途在于存储那些在topology生命周期中会产生变化且数量众多的文件（即blobs,在本文档中，blobs和“分布式缓存中文件”的意义等价），如位置数据、字典数据等。blobs的典型的应用场景包括短语识别、实体提取、文档分类、URL地址重写、定位/地址检测等。这些blobs数据的大小从几KB到几GB不等。对于那些不会动态更新的小数据集合，我们将其直接打包在topology的jar包中是可行的，但是对于那些大型的数据集合，打包再提交的启动时间将会非常大。在这个例子中，使用分布式缓存能大大提高topology的启动速度，特别要指出的是，同一个submitter提交的文件，会一直驻留在缓存中。这样的设计使得缓存中的数据可以重复利用。 在topology启动时，用户指定好哪些文件是topology所需要的。topology开始运行后，用户可以随时请求blobs并且更新其到新的版本。blobs的更新基于最终一致性模型（eventual consistency model）。如果topology想要知道其访问的文件的具体版本，这需要用户自己来实现相应信息的查询功能。缓存文件的置换使用 Least-Recently Used (LRU)算法，supervisor将基于这一算法对缓存文件进行置换。此外，blobs是可以压缩的，用户可以在使用其之前将其解压。 使用分布式缓存的动机 允许在topology之间共享blobs。 允许从命令行对blob进行更新。 分布式缓存的实现目前的BlobStore包括两个实现：LocalFsBlobStore 和HdfsBlobStore。详细接口请参考附录A。 ###LocalFsBlobStore Blobstore的本地文件系统实现如上面的时间线图中所示。blob的使用包括创建blob、下载blob以及在topology中使用blob。主要的步骤如下所示：####创建Blob的命令1storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1上面的命令创建了一个名为“key1”的blob，对应于文件readme.txt，对所有用户的访问权限为读(r)、写(w)、管理(a)。此外，该文件包含4个拷贝。#### topology提交及Blob映射用户可以使用下面的命令来提交topology。该命令包括拓扑映射配置。该配置包含两个键“key1”和“key2”，其中键“key1”具有一个名为“blob_file”的没有被压缩的本地文件名映射。12storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='&#123;\"key1\":&#123;\"localname\":\"blob_file\", \"uncompress\":\"false\"&#125;,\"key2\":&#123;&#125;&#125;'####Blob创建进程Blob通过ClientBlobStore接口进行创建。附录B中包含了ClientBlobStore中的所有接口。ClientBlobStore的一个具体实现是NimbusBlobStore。在使用本地文件系统的情况下，客户端调用nimbus来创建本地文件系统中的blob。nimbus使用本地文件系统实现来创建这些blob。当用户提交一个topology，包括jar、配置文件以及代码文件都被作为blob提交到blobstore中。####Supervisor下载blob最终，在topology运行时，同一个NimbusBlobStore thrift客户端上传的blob,通过nimbus分配到指定的supervisor，supervisor接受到分配指令后，即会下载对应的blob。supervisor通过NimbusBlobStore client直接下载topology的jar、conf blobs等。 ###HdfsBlobStore HdfsBlobStore 的blob创建和下载过程的实现和Local file system上的实现类似，唯一的区别在于支持对blobstore的多个拷贝。实现数据的多个拷贝是HDFS的天生技能，这也使得该模式下，我们不需要把blob的状态存储都在zookeeper中。另一方面，本地文件系统的blobstore需要将状态存储在zookeeper中，以便能和nimbus HA一起协同工作。Nimbus HA允许本地文件系统无缝地实现多拷贝这一特性，将状态存储在zookeeper的运行topology数据中，并在不同的Nimbus上同步这些blob。改模式下，最终supervisor使用HdfsClientBlobStore来与HdfsBlobStore 进行通信。 ##其他特性及相关文档 12storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='&#123;\"key1\":&#123;\"localname\":\"blob_file\", \"uncompress\":\"false\"&#125;,\"key2\":&#123;&#125;&#125;'###压缩BlobStore允许用户将uncompress配置项指定为true或false。这个配置可以在topology.blobstore.map中指定。上面的命令表示允许用户上传压缩文件，如tar/zip。在本地文件系统blobstore中，压缩的blob存储在nimbus节点上。本地化代码负责对blob进行解压，并将其存储在supervisor节点上。在用户逻辑开始执行之前，supervisor节点上blob的符号链接（Symbolic links）将会在worker中被创建。###本地文件名称映射除了压缩之外，在不同的supervisor节点上，本地化器(localizer)可在将blob映射为一个本地化的名称。###BlobStore的一些具体实现细节BlobStore基于哈希函数创建blobs. Blob通常存储在BlobStore指定的目录中（配置项blobstore.dir）。默认配置为storm.local.dir/blobs一旦提交了一个文件，BlobStore将读取configs，并为blob创建一个带有所有访问控制细节的元数据。在访问blob时，元数据通常用于授权。blob的哈希值基于blob数据的key和版本号生成。blob数据默认放置在storm.local.dir/blobs/data目录下。通常会生成放置在一个正数命名的目录来放置，如193,822。一旦topology启动，storm.conf, storm.ser和storm.code相关的blobs将会首先被下载，其他的命令行上上传的所有blob都使用本地化器来解压，并将它们映射到topology.blobstore.map指定的本地名称。supervisor通过检查版本的变化定期更新blob。动态地更新blob使它成为一个非常有用的特性。对于本地文件系统，supervisor节点上的分布式缓存被设置为1024mb（软限制），同时，根据LRU策略，将会在每600秒内清除任何超过软限制的内容。另一方面，HDFS BlobStore的实现通过消除在nimbus上存储blob的负担来更好地处理负载，从而避免它成为一个瓶颈。此外，它还提供了blob的无缝拷贝，本地文件系统BlobStore在复制blob时效率不高，并且受到nimbus的数量的限制。此外，在没有使用nimbus存储blob的情况下，supervisor直接与HDFS BlobStore进行通信，从而减少了对nimbus的负载和依赖性。 高可用的Nimbus问题描述目前，nimbus是一个在单个机器上运行的进程。在大多数情况下，nimbus失败是暂时的，并且可以通过执行监督的进程来重新启动。然而，当磁盘出现故障或者网络分区出现时，nimbus就会宕机。这种情况下，已经启动的topology能够正常运行，但是无法提交新的topology，且无法对运行着的topology进行kill、deactivated或activated等操作。此时，如果supervisor节点出现故障，则不会执行topology资源重新分配，从而导致性能下降或topology故障。当前的解决方案是，启动多台nimbus来避免单点故障。 高可用Nimbus的要求 增加nimbus的总体可用性。 允许nimbus主机随时离开并加入集群。一个新加入的主机应该自动加入可用的nimbus名单。 在nimbus失败的情况下，不需要进行拓扑重新提交。 任何正在执行的topology都不应丢失。 集群领导选举（Leader Election）nimbus包含以下接口： 12345678910111213141516171819202122232425262728293031public interface ILeaderElector &#123; /** * queue up for leadership lock. The call returns immediately and the caller * must check isLeader() to perform any leadership action. */ void addToLeaderLockQueue(); /** * Removes the caller from the leader lock queue. If the caller is leader * also releases the lock. */ void removeFromLeaderLockQueue(); /** * * @return true if the caller currently has the leader lock. */ boolean isLeader(); /** * * @return the current leader's address , throws exception if noone has has lock. */ InetSocketAddress getLeaderAddress(); /** * * @return list of current nimbus addresses, includes leader. */ List&lt;InetSocketAddress&gt; getAllNimbusAddresses();&#125; 一旦一个可用的nimbus出现，即调用addToLeaderLockQueue函数，将其加入可用nimbus名单。领导选举算法将从Queue中选出一个节点作为leader，若此时topology的代码、jar或者blob有丢失，则会从其他的正在运行的nimbus节点下载这些数据。 The first implementation will be Zookeeper based. If the zookeeper connection is lost/reset resulting in loss of lock or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the current status. The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not leader receives a request that only a leader can perform, it will throw a RunTimeException. Nimbus状态存储（未完待续）","comments":true,"tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Apache Storm","slug":"Apache-Storm","permalink":"http://blog.tangkailin.cn/tags/Apache-Storm/"},{"name":"翻译","slug":"翻译","permalink":"http://blog.tangkailin.cn/tags/翻译/"}]},{"title":"关于Big-data的一点总结","date":"2018-01-16T15:48:59.000Z","path":"2018/01/16/关于Big-data的一点总结/","text":"关于Big-data的一点总结 大数据知识架构大数据是一个被炒烂了的概念。但大数据到目前为止也没有一个很清晰的定义出现。这种现象在计算机科学中其实十分常见，许多常见的概念如数据结构等，都是没有清晰定义的。这是因为作为一个工科类的学科，许多概念并不需要像基础学科那样，给出严谨的定义；此外，由于从业人群基数大，五花八门的概念层出不穷，这些概念难免会出现重叠之处，这也是计算机学科概念难以定义的一个主要原因。大数据作为一个新出的概念，也避免不了这样的命运。本文希望能够基于我个人的理解以及知识储备，谈一谈我对大数据的简单理解。如果有不妥或者不当之处，希望各位指点，多多交流。大数据时代的到来，最重要的还是人们意识到了数据的重要性。正如我们以前没有意识到古董价值时，可以拿明代的碗来下面吃一样，只是原来我们没有发现数据的价值，没有搜集数据的意识而已，并非到了如今这个时代，才有这么多的数据出现。此外，大数据包含了哪些东西，如下图所示，我认为大数据理应包含三个部分。- 计算能力。- ​ 计算方法。- 其他辅助工具。首先是计算能力。应对海量数据的处理。有强大的计算能力支撑是很重要的一环。你1PB数据要跑一年才能跑出结果，再大的数据量也失去了意义。计算能力体现在快速的进行复杂计算（如Deep Learning）和快速的处理海量数据（如Distributed system）。要做到这两点并不容易，现行的一些方法大体思路分为两块：一块是并行化处理，包括并行计算和分布式计算等方式；另一块是针对特定应用使用特定的硬件设计来进行优化，包括高层次综合（如FPGA）以及TPU，寒武纪等。当然，前沿的还有革命性的量子计算，打破冯诺依曼体系的现代计算机结构的新型计算机。其次是计算方法，这里的计算方法指的是如何利用海量数据。我认为大数据量的出现和机器学习以及深度学习的大火是相辅相成的。有了大量数据，大规模的ML，DL才能得以实现；而有了DL和ML，海量数据的价值才能被充分的挖掘（希望未来能有更先进的技术挖掘出更多的信息。）最后，是大数据相关的一些技术。包括数据采集技术，如爬虫，传感器等等；数据检索技术，如ELK等；数据仓库技术，用于存储海量数据（多说一句，个人认为OLAP与OLTP唯一的区别在于，一个是动态的，一个是静态的。静态的OLTP只能存储数据，而动态的OLAP可以动态的对数据进行复杂的分析）；数据安全，如差分隐私等；以及大规模系统的运维、管理、组织、部署，如DevOps，容器技术等；都是为了方便技术人员或者从业人员更好的利用数据，更好的、更方便的使用系统而诞生的。这些技术并不一定都是用于大数据领域。 暂时写这么多，想到了再补充。","comments":true,"tags":[{"name":"Big data","slug":"Big-data","permalink":"http://blog.tangkailin.cn/tags/Big-data/"}]},{"title":"Serialize binary tree and Deserialize binary tree","date":"2017-09-01T15:30:03.000Z","path":"2017/09/01/Serialize binary tree and Deserialize binary tree/","text":"面试题一道。序列化与反序列化二叉树 牛客网真题链接 定义： 序列化： 将数据结构或对象转换成二进制串的过程。 反序列化：将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。 思路： 序列化：将二叉树转化成一个字符串，每个节点用分隔符分开。由于二叉树的结构可以由其先序遍历和中序遍历结果来决定。一开始想到用二者的结合，后来觉得太麻烦了。不如直接就用一个，只要把整棵树用特殊字符将空值填充称完全二叉树就行了。有了这个思想，其实不管什么遍历，都可以实现序列化。 反序列化：模拟遍历。 代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package interview.leetcode;/** * Created by 44931 on 2017/9/1. */public class Solution2 &#123; int index = -1; public static class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125; &#125; String Serialize(TreeNode root, StringBuffer stringBuffer) &#123; if (null == root) &#123; return String.valueOf(stringBuffer.append(&quot;#,&quot;)); &#125; stringBuffer.append(String.valueOf(root.val)+&quot;,&quot;); Serialize(root.left, stringBuffer); Serialize(root.right, stringBuffer); return String.valueOf(stringBuffer); &#125; String Serialize(TreeNode root) &#123; StringBuffer stringBuffer = new StringBuffer(); return Serialize(root, stringBuffer); &#125; TreeNode Deserialize(String str) &#123; index++; int len = str.length(); if(index &gt;= len)&#123; return null; &#125; String[] strr = str.split(&quot;,&quot;); TreeNode node = null; if(!strr[index].equals(&quot;#&quot;))&#123; node = new TreeNode(Integer.valueOf(strr[index])); node.left = Deserialize(str); node.right = Deserialize(str); &#125; return node; &#125; TreeNode Deserialize(String[] temp) &#123; index++; if (temp.length &lt; 1 || index &lt; 0 || temp.length &lt; index || temp[index].equals(&quot;#&quot;)) &#123; return null; &#125; TreeNode root = new TreeNode(Integer.valueOf(temp[index])); root.left = Deserialize(temp); root.right = Deserialize(temp); return root; &#125; public static void frontSearch(TreeNode node) &#123; if (node == null) &#123; return; &#125; System.out.println(node.val); frontSearch(node.left); frontSearch(node.right); &#125; public static void main(String[] args) &#123; Solution2 solution2 = new Solution2(); TreeNode n1 = new TreeNode(1); TreeNode n2 = new TreeNode(2); TreeNode n3 = new TreeNode(3); TreeNode n4 = new TreeNode(4); TreeNode n5 = new TreeNode(5); TreeNode n6 = new TreeNode(6); TreeNode n7 = new TreeNode(7); TreeNode n8 = new TreeNode(8); TreeNode n9 = new TreeNode(9); n1.left = n2; n1.right = n3; n2.left = n4; n2.right = n5; n3.left = n6; n3.right = n7; n4.left = n8; n4.right = n9; System.out.println(solution2.Serialize(n1)); frontSearch(solution2.Deserialize(solution2.Serialize(n1))); &#125;&#125;","comments":true,"tags":[{"name":"Binary tree","slug":"Binary-tree","permalink":"http://blog.tangkailin.cn/tags/Binary-tree/"}]},{"title":"leetcode-Letter Combinations of a Phone Number","date":"2017-09-01T09:12:38.000Z","path":"2017/09/01/leetcode-Letter Combinations of a Phone Number/","text":"很有意思的一道题。leetcode 原题：Given a digit string, return all possible letter combinations that the number could represent. A mapping of digit to letters (just like on the telephone buttons) is given below. Input:Digit string “23”Output: [“ad”, “ae”, “af”, “bd”, “be”, “bf”, “cd”, “ce”, “cf”].Note:Although the above answer is in lexicographical order, your answer could be in any order you want. 给定一个数字字符串，每个数字和字母的映射参考功能机的键盘。求有多少种不同的字母组合。 这个题其实很简单。每个数字对应3-4个字母，有n个数字，输出所有可能的组合。数字顺序不能调换，是一个组合问题而不是排列问题。很直观的就想到使用回溯，递归的方法。提取第i个数字对应的字母(使用字典咯)，针对每个字母递归进入第i+1个数字直到i=n,返回当前生成的字符串。 代码如下：12345678910111213141516171819202122232425class Solution &#123; public List&lt;String&gt; letterCombinations(String digits) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); if (digits.isEmpty()) &#123; return list; &#125; String[] dict = new String[]&#123;&quot; &quot;, &quot;&quot;, &quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;, &quot;jkl&quot;, &quot;mno&quot;, &quot;pqrs&quot;, &quot;tuv&quot;, &quot;wxyz&quot;&#125;; letterCombinationsRecursion(digits, dict, 0, &quot;&quot;, list); return list; &#125; private void letterCombinationsRecursion(String digits, String[] dict, int level, String out, ArrayList&lt;String&gt; list) &#123; if (level == digits.length()) &#123; list.add(out); &#125; else &#123; String str = dict[digits.charAt(level) - &apos;0&apos;]; for (int i=0; i&lt;str.length(); i++) &#123; out += str.charAt(i); letterCombinationsRecursion(digits, dict, level+1, out, list); out = out.substring(0,out.length()-1); &#125; &#125; &#125;&#125; 思考：看到这类限定性很强，或者枚举空间很小的题，总是会松懈，这样不行，做事要严谨一点。不要什么都想着用字典啊之类的暴力的方法，多想一些漂亮的数学上的解法。更多解法：here","comments":true,"tags":[{"name":"String","slug":"String","permalink":"http://blog.tangkailin.cn/tags/String/"},{"name":"leetcode","slug":"leetcode","permalink":"http://blog.tangkailin.cn/tags/leetcode/"}]},{"title":"Storm, Flink和spark Streaming中的backpressure机制比较","date":"2017-08-30T15:22:56.000Z","path":"2017/08/30/Storm, Flink和spark Streaming中的backpressure机制比较/","text":"Storm, Spark streaming, Flink中的backpressure机制比较。 backpressure是分布式流处理系统中一种有效的流量控制手段，为了防止负载过大，计算瓶颈等对整个系统带来的影响。backpressure基本上是每个流处理系统都应该考虑的一种流量控制手段。流量控制的方法有很多种，有兴趣的可以去看看网络中是怎么控制流量的。分布式流其实和网络流的本质是一样的。本文先分别讨论各个系统的backpressure机制，然后再总结三者的异同。 Stormstorm的backpressure功能流程如下：在worker中会有一个backpressure线程，实时的监控executor的receive queue或者worker的transfer queue的状态，一旦发现某一queue中的数据量超过一个阈值（由 backpressure.disruptor.high.watermark 配置，默认为0.9），即触发backpressure，此时backpressure线程会将这当前topology的信息写入zookeeper，watcher检测到zookeeper中的数据变化，则立马通知所有worker进入backpressure状态，上游spout停止发送数据,直到queue中的数据量低于一个阈值（由 backpressure.disruptor.low.watermark 配置，默认为0.4）。所有worker退出backpressure状态，spout正常发送数据。executor.clj中代码段：1234(if (and (not (.isFull transfer-queue)) (not throttle-on) (not reached-max-spout-pending)) (fast-list-iter [^ISpout spout spouts] (.nextTuple spout))))不了解disruptor queue在storm中的使用的可以参考：Understanding the Parallelism of a Storm Topologystorm中的backpressure还是相当暴力的。这样子其实整个系统都处在一个不稳定的状态，堵住了马上就不发送任何数据进来，直到疏通了，开始发送，然后慢慢堵住，直到触发backpressure。 storm backpressure具体内容参考：storm-886 FlinkFlink是新一代的流处理系统，其backpressure的实现方式相比于storm有很大的区别。其不再借助zookeeper或者其他的外部组件来实现backpressure。而是在其内部数据传输时用一种类似阻塞队列的方式很合理，很漂亮的实现了这一功能。具体流程已经有人写了很好的博文了,我就不再多次一举了。 Spark Streaming待研究。","comments":true,"tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Apache Storm","slug":"Apache-Storm","permalink":"http://blog.tangkailin.cn/tags/Apache-Storm/"},{"name":"Spark","slug":"Spark","permalink":"http://blog.tangkailin.cn/tags/Spark/"},{"name":"Flink","slug":"Flink","permalink":"http://blog.tangkailin.cn/tags/Flink/"}]},{"title":"leetcode-Longest Substring Without Repeating Characters","date":"2017-08-23T14:53:38.000Z","path":"2017/08/23/leetcode-Longest Substring Without Repeating Characters/","text":"寻找最长无重复子串的最优解法leetcode 原题：Given a string, find the length of the longest substring without repeating characters. Examples: Given “abcabcbb”, the answer is “abc”, which the length is 3. Given “bbbbb”, the answer is “b”, with the length of 1. Given “pwwkew”, the answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring. 网上有很多不同类型的解法，这里不做一一介绍，提供一个时间复杂度O(n)，空间复杂度O(1)的解法如下： 思路： 该问题的通常解法是遍历每个字符起始的子串，找出结果。在计算过程中使用hash存储中间结果。通过遍历的方式时间复杂度为O(n^2)。 可以考虑使用DP，因为对于字符串中的一个字符而言，如果其与 其前面字符结尾的最长不重复子串 中的字符没有重复的话。那么就可以将其加入到最长不重复子串中，则以当前字符结尾的子串长度为其前面找到的子串长度+1。若有重复，如果有重复，且重复位置在上一个最长子串起始位置之后，那么就与该起始位置之后的稍短的子串构成新的子串或者单独成一个新子串。dp表中记录以每个字符结尾的最长不重复子串。例如：有字符串abccd,有dp[0]=1,dp[1]=2,dp[2]=3。那么当找到第二个c时，此时以第一个c结尾的最长不重复子串长度为dp[2]=3（abc），发现有重复，则修改：dp[3]=1。找到d时，与前面的不重复子串没有相同字符，故dp[4]=2。这样的dp算法的时间复杂度为O(n^2)。因为每次我们都需要“回头”去寻找重复元素的位置。 怎么样才能不回头找呢？用hash咯。可以用hash记录元素是否出现过，key为元素值，value为元素上一次出现的位置。同时可以发现，其实我们不需要维护一张DP表，因为我们只需要记住以当前元素的前一个元素结尾的最长不重复子串的长度即可。所以可以进一步优化空间使用从O(n) -&gt; O(1)。 代码如下：1234567891011121314151617181920212223242526272829public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if (s == null || s.length() &lt;= 0) return 0; int maxLen = 1; int lastIndex = 0; int current = 1; HashMap&lt;Character,Integer&gt; map = new HashMap&lt;&gt;(); map.put(s.charAt(0),0); for (int i=1; i&lt;s.length(); i++) &#123; if (map.get(s.charAt(i)) == null) &#123; map.put(s.charAt(i), i); current++; &#125; else if (lastIndex &lt;= map.get(s.charAt(i))) &#123; current = i - map.get(s.charAt(i)); lastIndex = map.get(s.charAt(i))+1; map.put(s.charAt(i), i); &#125; else &#123; current++; map.put(s.charAt(i), i); &#125; if (current &gt; maxLen) &#123; maxLen = current; &#125; &#125; return maxLen; &#125;&#125; 参考：http://xfhnever.com/2014/10/30/algorithm-lnrs/","comments":true,"tags":[{"name":"String","slug":"String","permalink":"http://blog.tangkailin.cn/tags/String/"},{"name":"leetcode","slug":"leetcode","permalink":"http://blog.tangkailin.cn/tags/leetcode/"}]},{"title":"《二十二》观后感","date":"2017-08-19T02:32:50.000Z","path":"2017/08/19/《二十二观后感》/","text":"电影《二十二》观后感，有剧透，不喜勿看。 第一次看电影，全场安静无声音，灯亮后没有人离开，全部安静的坐着静静的听完片尾曲才慢慢离开。 如果说什么样的电影是好电影，我想这样的电影才是好电影。我不去谈论她是否符合一个纪录片应有的标准，也不care她是否有剧情，有高潮，有…… 这只是一部真实记录老人们现状的影片，没有任何多余的修饰，只是安静的记录着制作组所看到的真实的老人们的生活，没有对历史仇恨的刻意渲染，也不会卖弄老人们的眼泪来博取大家的同情。我想这才是这种类型的纪录片应有的样子。 老人们的生活很平静，安静的坐着，就能坐上一整天。回想起过去，表达出来的都是不愿再提及的态度。对于她们来说，过去的也许真的就已经过去了。她们会看着日本老兵的照片笑道：“日本人也老咯，头也秃了，都没有胡子咯。”， 仇恨随着时间的流逝已经慢慢变淡了，但她们想起坐着火车离她们远去的母亲，想起被日本人杀害的妈妈还是会泪流满面，经过岁月的洗礼，留下来的只有爱却没有恨，这是我在影片里看到的最美好的东西。 影片一开始，出现了一个韩国人，我看到时我以为是一个日本人，我还在想把一个日本人搞进来拍拍拍是不是不太合适，后来知道是韩国人就释怀了许多。等到影片快结束时，真的出现了一个日本人，在海南为这些奶奶们尽着自己的力。这时我却没有什么太多的抵触了。我想看完这部电影真的能化解一个人身上的戾气。是我自己庸俗了。 看到一半时我会想，为什么这些老人的物质生活条件看上去并不是那么的好，是我们的国家没有给予他们应有的帮助吗？但我看到海南那位去世的老奶奶房间里留下来的志愿者们送来的棉被等等东西的时候，我才终于明白过来，这些老人需要的并不是多么优异的物质条件，他们已经平淡的过了这么多年，他们需要的只是没有人打扰的安静的生活。 影片结尾时，片头出来的葬礼主持人，呼应了32年这个伏笔。他从82开始为这些受害者们与日本政府斗争，这么多年来已经麻木了，觉得自己年轻时太天真，只想着为她们讨回公道，现在都看淡了。其实我个人对于去日本打官司这种与虎谋皮的行为是不太赞同的。我们的斗争，是要让人类记住这样的过去，让不管是日本人还是其他国家的人也好，看清这件事里面的是非曲直。就算日本人不道歉，他们心里也应该很清楚，谁对谁错。对的就是对的，错的就是错的。但对于我们自己而言，中国人，我们要知道的不应该仅仅是日本人怎么怎么样。更应该知道我们自己人是怎么对待这些受害者的，记住那个日本人的儿子，七十年没有结婚的老爷爷。这是我们自己人对自己人的歧视，是我们这个民族自身所存在的问题。这些才是我们应该要重视起来的。反省自己，更好的面向未来。 2017年，据制作组的统计，二十二人中现在已经只剩下八人，很可能在未来的几年里。这个数字将会变为0。这些数字会被写进书里，这段过去会被称为了历史。很多很多年后，当人们看到这些数字，这段历史的时候并不会有太多的感悟和感觉。就像我们现在看到古代的大屠杀时，没有任何感觉，十万，百万只是一个冰冷的数字而已。我们更应该做的，是留下在心里传承下去的东西，文化。这才是我们这个民族的灵魂。 很讨厌那帮逼逼弱肉强食，适者生存的人。人类社会发展的意义何在？如果我们社会的发展不能带来全人类生活的共同改善，不能带来全人类的平等和自由，却还是和原始的自然一样弱肉强食，两级分化，贫富悬殊，那我们人类为之奋斗的到底又是什么呢？ 我们的社会的形成，组织和发展无法逃避自然规律的约束，但我们至少应该让自己和纯粹的自然社会区分开来。 愿这些老奶奶，下辈子做一个快乐的女孩。 PS：感触很深的一点，这些老奶奶差不多都是1920年代出生的人，抗战时她们大部分才只有13,14岁。这些是活着的，之前死去的那些20多岁，30多岁的。他们也许受到更多更久的欺凌。我们看到的只是冰山一角。","comments":true,"tags":[{"name":"影评","slug":"影评","permalink":"http://blog.tangkailin.cn/tags/影评/"}]},{"title":"spring学习笔记(1)-基本概念","date":"2017-08-17T04:43:56.000Z","path":"2017/08/17/Spring学习(1)/","text":"Spring基本概念 Inversion of Control And Dependency InjectionIOCIOC是一种设计的思想，在程序设计时并不直接在某个对象内部去new一个其他对象，而是使用一个IOC容器来创建这些对象。使用IOC容器来控制对象，主要控制外部资源的获取。 所谓inversion，指的就是由容器来帮忙创建及注入依赖对象。依赖对象的获取由主动变为被动，这是一个反转。应用被动的等待IOC容器来创建注入它所需要的资源。 DI应用依赖于IOC容器。因为其需要IOC容器来提供外部资源。IOC将这些外部资源注入某个对象。 其实依赖注入只是一种装配对象的手段，设计的类结构才是基础，如果设计的类结构不支持依赖注入，Spring IoC容器也注入不了任何东西。 循环依赖很显然，循环依赖是指的，当创建A时，需要B，就去创建B，创建B时发现需要A。此时就有问题了，因为A还没创建完成，然后就去新建一个A，这样就会陷入死循环。 Aspect Orient Programming 面向切面编程http://jinnianshilongnian.iteye.com/blog/1418596 在进行OOP开发时，都是基于对组件（比如类）进行开发，然后对组件进行组合，OOP最大问题就是无法解耦组件进行开发，比如我们上边举例，而AOP就是为了克服这个问题而出现的，它来进行这种耦合的分离。AOP为开发者提供一种进行横切关注点。（比如日志关注点横切了支付关注点） 分离并织入的机制，把横切关注点分离，然后通过某种技术织入到系统中，从而无耦合的完成了我们的功能。 关注点：可以认为是所关注的任何东西，比如上边的支付组件；关注点分离：将问题细化从而单独部分，即可以理解为不可再分割的组件，如上边的日志组件和支付组件；横切关注点：一个组件无法完成需要的功能，需要其他组件协作完成，如日志组件横切于支付组件；","comments":true,"tags":[{"name":"spring","slug":"spring","permalink":"http://blog.tangkailin.cn/tags/spring/"},{"name":"java","slug":"java","permalink":"http://blog.tangkailin.cn/tags/java/"}]},{"title":"storm资源汇总","date":"2017-08-10T12:36:59.000Z","path":"2017/08/10/storm资源汇总/","text":"Apache storm 的一些网络资源汇总。 storm入门官网地址 源码地址 storm@twitter 部署教程 Default configuration storm配置项 相关博客 fxjwind的博文，有部分源码解读是老版本的storm。1.0.X的版本之后会有一些变动的地方（如worker如何接收数据）。但作为storm源码阅读的辅助，这些博文还是相当牛逼的。向fxjwind大神致敬。 http://www.cnblogs.com/fxjwind/category/455987.html 徽沪一郎 Blog - Apache Storm 源码走读系列。很cool的博客。 http://www.cnblogs.com/hseagle/p/3756862.html Some important concept Understanding the Internal Message Buffers of Storm Understanding the Parallelism of a Storm Topology Guaranteeing Message Processing At least once storm trident Storm Metrics DRPC详解，中文版译文 storm在zookeeper上的目录结构 kafka原理 storm schedulerstorm 有多个不同的调度器。包括 DefaultScheduler, IsolationScheduler, MultitenantScheduler, ResourceAwareScheduler等。 相关项目 zookeeper thrift，推荐董的博客。 Dynamic Resource Scheduling，该项目支持storm在运行时动态调整资源。 storm on yarn，利用资源调度框架YARN来调度storm资源。 storm on docker,在docker swarm集群里部署storm。 Jstorm，alibaba开源的Java版本及优化版本的类storm流处理系统。 感谢感谢troyding，他的文章让我有了写这篇博客的想法。我的文章是基于他的工作完成的。本文将持续更新。","comments":true,"tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Apache Storm","slug":"Apache-Storm","permalink":"http://blog.tangkailin.cn/tags/Apache-Storm/"}]}]