[{"title":"Storm中的分布式缓存","date":"2018-01-20T02:50:56.000Z","path":"2018/01/20/Storm中的分布式缓存/","text":"Storm中的分布式缓存 原文地址：http://storm.apache.org/releases/1.1.1/distcache-blobstore.html 翻译水平有限，欢迎各位指正。 Storm 分布式缓存API Storm中的分布式缓存的主要用途在于存储那些在topology生命周期中会产生变化且数量众多的文件（即blobs,在本文档中，blobs和“分布式缓存中文件”的意义等价），如位置数据、字典数据等。blobs的典型的应用场景包括短语识别、实体提取、文档分类、URL地址重写、定位/地址检测等。这些blobs数据的大小从几KB到几GB不等。对于那些不会动态更新的小数据集合，我们将其直接打包在topology的jar包中是可行的，但是对于那些大型的数据集合，打包再提交的启动时间将会非常大。在这个例子中，使用分布式缓存能大大提高topology的启动速度，特别要指出的是，同一个submitter提交的文件，会一直驻留在缓存中。这样的设计使得缓存中的数据可以重复利用。 在topology启动时，用户指定好哪些文件是topology所需要的。topology开始运行后，用户可以随时请求blobs并且更新其到新的版本。blobs的更新基于最终一致性模型（eventual consistency model）。如果topology想要知道其访问的文件的具体版本，这需要用户自己来实现相应信息的查询功能。缓存文件的置换使用 Least-Recently Used (LRU)算法，supervisor将基于这一算法对缓存文件进行置换。此外，blobs是可以压缩的，用户可以在使用其之前将其解压。 使用分布式缓存的动机 允许在topology之间共享blobs。 允许从命令行对blob进行更新。 分布式缓存的实现目前的BlobStore包括两个实现：LocalFsBlobStore 和HdfsBlobStore。详细接口请参考附录A。 ###LocalFsBlobStore Blobstore的本地文件系统实现如上面的时间线图中所示。blob的使用包括创建blob、下载blob以及在topology中使用blob。主要的步骤如下所示：####创建Blob的命令1storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1上面的命令创建了一个名为“key1”的blob，对应于文件readme.txt，对所有用户的访问权限为读(r)、写(w)、管理(a)。此外，该文件包含4个拷贝。#### topology提交及Blob映射用户可以使用下面的命令来提交topology。该命令包括拓扑映射配置。该配置包含两个键“key1”和“key2”，其中键“key1”具有一个名为“blob_file”的没有被压缩的本地文件名映射。12storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='&#123;\"key1\":&#123;\"localname\":\"blob_file\", \"uncompress\":\"false\"&#125;,\"key2\":&#123;&#125;&#125;'####Blob创建进程Blob通过ClientBlobStore接口进行创建。附录B中包含了ClientBlobStore中的所有接口。ClientBlobStore的一个具体实现是NimbusBlobStore。在使用本地文件系统的情况下，客户端调用nimbus来创建本地文件系统中的blob。nimbus使用本地文件系统实现来创建这些blob。当用户提交一个topology，包括jar、配置文件以及代码文件都被作为blob提交到blobstore中。####Supervisor下载blob最终，在topology运行时，同一个NimbusBlobStore thrift客户端上传的blob,通过nimbus分配到指定的supervisor，supervisor接受到分配指令后，即会下载对应的blob。supervisor通过NimbusBlobStore client直接下载topology的jar、conf blobs等。 ###HdfsBlobStore HdfsBlobStore 的blob创建和下载过程的实现和Local file system上的实现类似，唯一的区别在于支持对blobstore的多个拷贝。实现数据的多个拷贝是HDFS的天生技能，这也使得该模式下，我们不需要把blob的状态存储都在zookeeper中。另一方面，本地文件系统的blobstore需要将状态存储在zookeeper中，以便能和nimbus HA一起协同工作。Nimbus HA允许本地文件系统无缝地实现多拷贝这一特性，将状态存储在zookeeper的运行topology数据中，并在不同的Nimbus上同步这些blob。改模式下，最终supervisor使用HdfsClientBlobStore来与HdfsBlobStore 进行通信。 ##其他特性及相关文档 12storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='&#123;\"key1\":&#123;\"localname\":\"blob_file\", \"uncompress\":\"false\"&#125;,\"key2\":&#123;&#125;&#125;'###压缩BlobStore允许用户将uncompress配置项指定为true或false。这个配置可以在topology.blobstore.map中指定。上面的命令表示允许用户上传压缩文件，如tar/zip。在本地文件系统blobstore中，压缩的blob存储在nimbus节点上。本地化代码负责对blob进行解压，并将其存储在supervisor节点上。在用户逻辑开始执行之前，supervisor节点上blob的符号链接（Symbolic links）将会在worker中被创建。###本地文件名称映射除了压缩之外，在不同的supervisor节点上，本地化器(localizer)可在将blob映射为一个本地化的名称。###BlobStore的一些具体实现细节BlobStore基于哈希函数创建blobs. Blob通常存储在BlobStore指定的目录中（配置项blobstore.dir）。默认配置为storm.local.dir/blobs一旦提交了一个文件，BlobStore将读取configs，并为blob创建一个带有所有访问控制细节的元数据。在访问blob时，元数据通常用于授权。blob的哈希值基于blob数据的key和版本号生成。blob数据默认放置在storm.local.dir/blobs/data目录下。通常会生成放置在一个正数命名的目录来放置，如193,822。一旦topology启动，storm.conf, storm.ser和storm.code相关的blobs将会首先被下载，其他的命令行上上传的所有blob都使用本地化器来解压，并将它们映射到topology.blobstore.map指定的本地名称。supervisor通过检查版本的变化定期更新blob。动态地更新blob使它成为一个非常有用的特性。对于本地文件系统，supervisor节点上的分布式缓存被设置为1024mb（软限制），同时，根据LRU策略，将会在每600秒内清除任何超过软限制的内容。另一方面，HDFS BlobStore的实现通过消除在nimbus上存储blob的负担来更好地处理负载，从而避免它成为一个瓶颈。此外，它还提供了blob的无缝拷贝，本地文件系统BlobStore在复制blob时效率不高，并且受到nimbus的数量的限制。此外，在没有使用nimbus存储blob的情况下，supervisor直接与HDFS BlobStore进行通信，从而减少了对nimbus的负载和依赖性。 高可用的Nimbus问题描述：目前，nimbus是一个在单个机器上运行的进程。在大多数情况下，nimbus失败是暂时的，并且可以通过执行监督的进程来重新启动。然而，当磁盘出现故障或者网络分区出现时，nimbus就会宕机。这种情况下，已经启动的topology能够正常运行，但是无法提交新的topology，且无法对运行着的topology进行kill、deactivated或activated等操作。此时，如果supervisor节点出现故障，则不会执行topology资源重新分配，从而导致性能下降或topology故障。当前的解决方案是，启动多台nimbus来避免单点故障。 高可用Nimbus的要求： 增加nimbus的总体可用性。 允许nimbus主机随时离开并加入集群。一个新加入的主机应该自动加入可用的nimbus名单。 在nimbus失败的情况下，不需要进行拓扑重新提交。 任何正在执行的topology都不应丢失。 集群领导选举（Leader Election）nimbus包含以下接口： 12345678910111213141516171819202122232425262728293031public interface ILeaderElector &#123; /** * queue up for leadership lock. The call returns immediately and the caller * must check isLeader() to perform any leadership action. */ void addToLeaderLockQueue(); /** * Removes the caller from the leader lock queue. If the caller is leader * also releases the lock. */ void removeFromLeaderLockQueue(); /** * * @return true if the caller currently has the leader lock. */ boolean isLeader(); /** * * @return the current leader's address , throws exception if noone has has lock. */ InetSocketAddress getLeaderAddress(); /** * * @return list of current nimbus addresses, includes leader. */ List&lt;InetSocketAddress&gt; getAllNimbusAddresses();&#125; 一旦一个可用的nimbus出现，即调用addToLeaderLockQueue函数，将其加入可用nimbus名单。领导选举算法将从Queue中选出一个节点作为leader，若此时topology的代码、jar或者blob有丢失，则会从其他的正在运行的nimbus节点下载这些数据。 （未完待续）","tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Storm","slug":"Storm","permalink":"http://blog.tangkailin.cn/tags/Storm/"}]},{"title":"关于Big-data的一点总结","date":"2018-01-16T15:48:59.000Z","path":"2018/01/16/关于Big-data的一点总结/","text":"关于Big-data的一点总结 大数据知识架构大数据是一个被炒烂了的概念。但大数据到目前为止也没有一个很清晰的定义出现。这种现象在计算机科学中其实十分常见，许多常见的概念如数据结构等，都是没有清晰定义的。这是因为作为一个工科类的学科，许多概念并不需要像基础学科那样，给出严谨的定义；此外，由于从业人群基数大，五花八门的概念层出不穷，这些概念难免会出现重叠之处，这也是计算机学科概念难以定义的一个主要原因。大数据作为一个新出的概念，也避免不了这样的命运。本文希望能够基于我个人的理解以及知识储备，谈一谈我对大数据的简单理解。如果有不妥或者不当之处，希望各位指点，多多交流。大数据时代的到来，最重要的还是人们意识到了数据的重要性。正如我们以前没有意识到古董价值时，可以拿明代的碗来下面吃一样，只是原来我们没有发现数据的价值，没有搜集数据的意识而已，并非到了如今这个时代，才有这么多的数据出现。此外，大数据包含了哪些东西，如下图所示，我认为大数据理应包含三个部分。- 计算能力。- ​ 计算方法。- 其他辅助工具。首先是计算能力。应对海量数据的处理。有强大的计算能力支撑是很重要的一环。你1PB数据要跑一年才能跑出结果，再大的数据量也失去了意义。计算能力体现在快速的进行复杂计算（如Deep Learning）和快速的处理海量数据（如Distributed system）。要做到这两点并不容易，现行的一些方法大体思路分为两块：一块是并行化处理，包括并行计算和分布式计算等方式；另一块是针对特定应用使用特定的硬件设计来进行优化，包括高层次综合（如FPGA）以及TPU，寒武纪等。当然，前沿的还有革命性的量子计算，打破冯诺依曼体系的现代计算机结构的新型计算机。其次是计算方法，这里的计算方法指的是如何利用海量数据。我认为大数据量的出现和机器学习以及深度学习的大火是相辅相成的。有了大量数据，大规模的ML，DL才能得以实现；而有了DL和ML，海量数据的价值才能被充分的挖掘（希望未来能有更先进的技术挖掘出更多的信息。）最后，是大数据相关的一些技术。包括数据采集技术，如爬虫，传感器等等；数据检索技术，如ELK等；数据仓库技术，用于存储海量数据（多说一句，个人认为OLAP与OLTP唯一的区别在于，一个是动态的，一个是静态的。静态的OLTP只能存储数据，而动态的OLAP可以动态的对数据进行复杂的分析）；数据安全，如差分隐私等；以及大规模系统的运维、管理、组织、部署，如DevOps，容器技术等；都是为了方便技术人员或者从业人员更好的利用数据，更好的、更方便的使用系统而诞生的。这些技术并不一定都是用于大数据领域。 暂时写这么多，想到了再补充。","tags":[{"name":"Big data","slug":"Big-data","permalink":"http://blog.tangkailin.cn/tags/Big-data/"}]},{"title":"Serialize binary tree and Deserialize binary tree","date":"2017-09-01T15:30:03.000Z","path":"2017/09/01/Serialize binary tree and Deserialize binary tree/","text":"面试题一道。序列化与反序列化二叉树 牛客网真题链接 定义： 序列化： 将数据结构或对象转换成二进制串的过程。 反序列化：将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。 思路： 序列化：将二叉树转化成一个字符串，每个节点用分隔符分开。由于二叉树的结构可以由其先序遍历和中序遍历结果来决定。一开始想到用二者的结合，后来觉得太麻烦了。不如直接就用一个，只要把整棵树用特殊字符将空值填充称完全二叉树就行了。有了这个思想，其实不管什么遍历，都可以实现序列化。 反序列化：模拟遍历。 代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package interview.leetcode;/** * Created by 44931 on 2017/9/1. */public class Solution2 &#123; int index = -1; public static class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125; &#125; String Serialize(TreeNode root, StringBuffer stringBuffer) &#123; if (null == root) &#123; return String.valueOf(stringBuffer.append(&quot;#,&quot;)); &#125; stringBuffer.append(String.valueOf(root.val)+&quot;,&quot;); Serialize(root.left, stringBuffer); Serialize(root.right, stringBuffer); return String.valueOf(stringBuffer); &#125; String Serialize(TreeNode root) &#123; StringBuffer stringBuffer = new StringBuffer(); return Serialize(root, stringBuffer); &#125; TreeNode Deserialize(String str) &#123; index++; int len = str.length(); if(index &gt;= len)&#123; return null; &#125; String[] strr = str.split(&quot;,&quot;); TreeNode node = null; if(!strr[index].equals(&quot;#&quot;))&#123; node = new TreeNode(Integer.valueOf(strr[index])); node.left = Deserialize(str); node.right = Deserialize(str); &#125; return node; &#125; TreeNode Deserialize(String[] temp) &#123; index++; if (temp.length &lt; 1 || index &lt; 0 || temp.length &lt; index || temp[index].equals(&quot;#&quot;)) &#123; return null; &#125; TreeNode root = new TreeNode(Integer.valueOf(temp[index])); root.left = Deserialize(temp); root.right = Deserialize(temp); return root; &#125; public static void frontSearch(TreeNode node) &#123; if (node == null) &#123; return; &#125; System.out.println(node.val); frontSearch(node.left); frontSearch(node.right); &#125; public static void main(String[] args) &#123; Solution2 solution2 = new Solution2(); TreeNode n1 = new TreeNode(1); TreeNode n2 = new TreeNode(2); TreeNode n3 = new TreeNode(3); TreeNode n4 = new TreeNode(4); TreeNode n5 = new TreeNode(5); TreeNode n6 = new TreeNode(6); TreeNode n7 = new TreeNode(7); TreeNode n8 = new TreeNode(8); TreeNode n9 = new TreeNode(9); n1.left = n2; n1.right = n3; n2.left = n4; n2.right = n5; n3.left = n6; n3.right = n7; n4.left = n8; n4.right = n9; System.out.println(solution2.Serialize(n1)); frontSearch(solution2.Deserialize(solution2.Serialize(n1))); &#125;&#125;","tags":[{"name":"Binary tree","slug":"Binary-tree","permalink":"http://blog.tangkailin.cn/tags/Binary-tree/"}]},{"title":"leetcode-Letter Combinations of a Phone Number","date":"2017-09-01T09:12:38.000Z","path":"2017/09/01/leetcode-Letter Combinations of a Phone Number/","text":"很有意思的一道题。leetcode 原题：Given a digit string, return all possible letter combinations that the number could represent. A mapping of digit to letters (just like on the telephone buttons) is given below. Input:Digit string “23”Output: [“ad”, “ae”, “af”, “bd”, “be”, “bf”, “cd”, “ce”, “cf”].Note:Although the above answer is in lexicographical order, your answer could be in any order you want. 给定一个数字字符串，每个数字和字母的映射参考功能机的键盘。求有多少种不同的字母组合。 这个题其实很简单。每个数字对应3-4个字母，有n个数字，输出所有可能的组合。数字顺序不能调换，是一个组合问题而不是排列问题。很直观的就想到使用回溯，递归的方法。提取第i个数字对应的字母(使用字典咯)，针对每个字母递归进入第i+1个数字直到i=n,返回当前生成的字符串。 代码如下：12345678910111213141516171819202122232425class Solution &#123; public List&lt;String&gt; letterCombinations(String digits) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); if (digits.isEmpty()) &#123; return list; &#125; String[] dict = new String[]&#123;&quot; &quot;, &quot;&quot;, &quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;, &quot;jkl&quot;, &quot;mno&quot;, &quot;pqrs&quot;, &quot;tuv&quot;, &quot;wxyz&quot;&#125;; letterCombinationsRecursion(digits, dict, 0, &quot;&quot;, list); return list; &#125; private void letterCombinationsRecursion(String digits, String[] dict, int level, String out, ArrayList&lt;String&gt; list) &#123; if (level == digits.length()) &#123; list.add(out); &#125; else &#123; String str = dict[digits.charAt(level) - &apos;0&apos;]; for (int i=0; i&lt;str.length(); i++) &#123; out += str.charAt(i); letterCombinationsRecursion(digits, dict, level+1, out, list); out = out.substring(0,out.length()-1); &#125; &#125; &#125;&#125; 思考：看到这类限定性很强，或者枚举空间很小的题，总是会松懈，这样不行，做事要严谨一点。不要什么都想着用字典啊之类的暴力的方法，多想一些漂亮的数学上的解法。更多解法：here","tags":[{"name":"String","slug":"String","permalink":"http://blog.tangkailin.cn/tags/String/"},{"name":"leetcode","slug":"leetcode","permalink":"http://blog.tangkailin.cn/tags/leetcode/"}]},{"title":"Storm, Flink和spark Streaming中的backpressure机制比较","date":"2017-08-30T15:22:56.000Z","path":"2017/08/30/Storm, Flink和spark Streaming中的backpressure机制比较/","text":"Storm, Spark streaming, Flink中的backpressure机制比较。 backpressure是分布式流处理系统中一种有效的流量控制手段，为了防止负载过大，计算瓶颈等对整个系统带来的影响。backpressure基本上是每个流处理系统都应该考虑的一种流量控制手段。流量控制的方法有很多种，有兴趣的可以去看看网络中是怎么控制流量的。分布式流其实和网络流的本质是一样的。本文先分别讨论各个系统的backpressure机制，然后再总结三者的异同。 Stormstorm的backpressure功能流程如下：在worker中会有一个backpressure线程，实时的监控executor的receive queue或者worker的transfer queue的状态，一旦发现某一queue中的数据量超过一个阈值（由 backpressure.disruptor.high.watermark 配置，默认为0.9），即触发backpressure，此时backpressure线程会将这当前topology的信息写入zookeeper，watcher检测到zookeeper中的数据变化，则立马通知所有worker进入backpressure状态，上游spout停止发送数据,直到queue中的数据量低于一个阈值（由 backpressure.disruptor.low.watermark 配置，默认为0.4）。所有worker退出backpressure状态，spout正常发送数据。executor.clj中代码段：1234(if (and (not (.isFull transfer-queue)) (not throttle-on) (not reached-max-spout-pending)) (fast-list-iter [^ISpout spout spouts] (.nextTuple spout))))不了解disruptor queue在storm中的使用的可以参考：Understanding the Parallelism of a Storm Topologystorm中的backpressure还是相当暴力的。这样子其实整个系统都处在一个不稳定的状态，堵住了马上就不发送任何数据进来，直到疏通了，开始发送，然后慢慢堵住，直到触发backpressure。 storm backpressure具体内容参考：storm-886 FlinkFlink是新一代的流处理系统，其backpressure的实现方式相比于storm有很大的区别。其不再借助zookeeper或者其他的外部组件来实现backpressure。而是在其内部数据传输时用一种类似阻塞队列的方式很合理，很漂亮的实现了这一功能。具体流程已经有人写了很好的博文了,我就不再多次一举了。 Spark Streaming待研究。","tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Storm","slug":"Storm","permalink":"http://blog.tangkailin.cn/tags/Storm/"},{"name":"Spark","slug":"Spark","permalink":"http://blog.tangkailin.cn/tags/Spark/"},{"name":"Flink","slug":"Flink","permalink":"http://blog.tangkailin.cn/tags/Flink/"}]},{"title":"leetcode-Longest Substring Without Repeating Characters","date":"2017-08-23T14:53:38.000Z","path":"2017/08/23/leetcode-Longest Substring Without Repeating Characters/","text":"寻找最长无重复子串的最优解法leetcode 原题：Given a string, find the length of the longest substring without repeating characters. Examples: Given “abcabcbb”, the answer is “abc”, which the length is 3. Given “bbbbb”, the answer is “b”, with the length of 1. Given “pwwkew”, the answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring. 网上有很多不同类型的解法，这里不做一一介绍，提供一个时间复杂度O(n)，空间复杂度O(1)的解法如下： 思路： 该问题的通常解法是遍历每个字符起始的子串，找出结果。在计算过程中使用hash存储中间结果。通过遍历的方式时间复杂度为O(n^2)。 可以考虑使用DP，因为对于字符串中的一个字符而言，如果其与 其前面字符结尾的最长不重复子串 中的字符没有重复的话。那么就可以将其加入到最长不重复子串中，则以当前字符结尾的子串长度为其前面找到的子串长度+1。若有重复，如果有重复，且重复位置在上一个最长子串起始位置之后，那么就与该起始位置之后的稍短的子串构成新的子串或者单独成一个新子串。dp表中记录以每个字符结尾的最长不重复子串。例如：有字符串abccd,有dp[0]=1,dp[1]=2,dp[2]=3。那么当找到第二个c时，此时以第一个c结尾的最长不重复子串长度为dp[2]=3（abc），发现有重复，则修改：dp[3]=1。找到d时，与前面的不重复子串没有相同字符，故dp[4]=2。这样的dp算法的时间复杂度为O(n^2)。因为每次我们都需要“回头”去寻找重复元素的位置。 怎么样才能不回头找呢？用hash咯。可以用hash记录元素是否出现过，key为元素值，value为元素上一次出现的位置。同时可以发现，其实我们不需要维护一张DP表，因为我们只需要记住以当前元素的前一个元素结尾的最长不重复子串的长度即可。所以可以进一步优化空间使用从O(n) -&gt; O(1)。 代码如下：1234567891011121314151617181920212223242526272829public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if (s == null || s.length() &lt;= 0) return 0; int maxLen = 1; int lastIndex = 0; int current = 1; HashMap&lt;Character,Integer&gt; map = new HashMap&lt;&gt;(); map.put(s.charAt(0),0); for (int i=1; i&lt;s.length(); i++) &#123; if (map.get(s.charAt(i)) == null) &#123; map.put(s.charAt(i), i); current++; &#125; else if (lastIndex &lt;= map.get(s.charAt(i))) &#123; current = i - map.get(s.charAt(i)); lastIndex = map.get(s.charAt(i))+1; map.put(s.charAt(i), i); &#125; else &#123; current++; map.put(s.charAt(i), i); &#125; if (current &gt; maxLen) &#123; maxLen = current; &#125; &#125; return maxLen; &#125;&#125; 参考：http://xfhnever.com/2014/10/30/algorithm-lnrs/","tags":[{"name":"String","slug":"String","permalink":"http://blog.tangkailin.cn/tags/String/"},{"name":"leetcode","slug":"leetcode","permalink":"http://blog.tangkailin.cn/tags/leetcode/"}]},{"title":"《二十二》观后感","date":"2017-08-19T02:32:50.000Z","path":"2017/08/19/《二十二观后感》/","text":"电影《二十二》观后感，有剧透，不喜勿看。 第一次看电影，全场安静无声音，灯亮后没有人离开，全部安静的坐着静静的听完片尾曲才慢慢离开。 如果说什么样的电影是好电影，我想这样的电影才是好电影。我不去谈论她是否符合一个纪录片应有的标准，也不care她是否有剧情，有高潮，有…… 这只是一部真实记录老人们现状的影片，没有任何多余的修饰，只是安静的记录着制作组所看到的真实的老人们的生活，没有对历史仇恨的刻意渲染，也不会卖弄老人们的眼泪来博取大家的同情。我想这才是这种类型的纪录片应有的样子。 老人们的生活很平静，安静的坐着，就能坐上一整天。回想起过去，表达出来的都是不愿再提及的态度。对于她们来说，过去的也许真的就已经过去了。她们会看着日本老兵的照片笑道：“日本人也老咯，头也秃了，都没有胡子咯。”， 仇恨随着时间的流逝已经慢慢变淡了，但她们想起坐着火车离她们远去的母亲，想起被日本人杀害的妈妈还是会泪流满面，经过岁月的洗礼，留下来的只有爱却没有恨，这是我在影片里看到的最美好的东西。 影片一开始，出现了一个韩国人，我看到时我以为是一个日本人，我还在想把一个日本人搞进来拍拍拍是不是不太合适，后来知道是韩国人就释怀了许多。等到影片快结束时，真的出现了一个日本人，在海南为这些奶奶们尽着自己的力。这时我却没有什么太多的抵触了。我想看完这部电影真的能化解一个人身上的戾气。是我自己庸俗了。 看到一半时我会想，为什么这些老人的物质生活条件看上去并不是那么的好，是我们的国家没有给予他们应有的帮助吗？但我看到海南那位去世的老奶奶房间里留下来的志愿者们送来的棉被等等东西的时候，我才终于明白过来，这些老人需要的并不是多么优异的物质条件，他们已经平淡的过了这么多年，他们需要的只是没有人打扰的安静的生活。 影片结尾时，片头出来的葬礼主持人，呼应了32年这个伏笔。他从82开始为这些受害者们与日本政府斗争，这么多年来已经麻木了，觉得自己年轻时太天真，只想着为她们讨回公道，现在都看淡了。其实我个人对于去日本打官司这种与虎谋皮的行为是不太赞同的。我们的斗争，是要让人类记住这样的过去，让不管是日本人还是其他国家的人也好，看清这件事里面的是非曲直。就算日本人不道歉，他们心里也应该很清楚，谁对谁错。对的就是对的，错的就是错的。但对于我们自己而言，中国人，我们要知道的不应该仅仅是日本人怎么怎么样。更应该知道我们自己人是怎么对待这些受害者的，记住那个日本人的儿子，七十年没有结婚的老爷爷。这是我们自己人对自己人的歧视，是我们这个民族自身所存在的问题。这些才是我们应该要重视起来的。反省自己，更好的面向未来。 2017年，据制作组的统计，二十二人中现在已经只剩下八人，很可能在未来的几年里。这个数字将会变为0。这些数字会被写进书里，这段过去会被称为了历史。很多很多年后，当人们看到这些数字，这段历史的时候并不会有太多的感悟和感觉。就像我们现在看到古代的大屠杀时，没有任何感觉，十万，百万只是一个冰冷的数字而已。我们更应该做的，是留下在心里传承下去的东西，文化。这才是我们这个民族的灵魂。 很讨厌那帮逼逼弱肉强食，适者生存的人。人类社会发展的意义何在？如果我们社会的发展不能带来全人类生活的共同改善，不能带来全人类的平等和自由，却还是和原始的自然一样弱肉强食，两级分化，贫富悬殊，那我们人类为之奋斗的到底又是什么呢？ 我们的社会的形成，组织和发展无法逃避自然规律的约束，但我们至少应该让自己和纯粹的自然社会区分开来。 愿这些老奶奶，下辈子做一个快乐的女孩。 PS：感触很深的一点，这些老奶奶差不多都是1920年代出生的人，抗战时她们大部分才只有13,14岁。这些是活着的，之前死去的那些20多岁，30多岁的。他们也许受到更多更久的欺凌。我们看到的只是冰山一角。","tags":[{"name":"影评","slug":"影评","permalink":"http://blog.tangkailin.cn/tags/影评/"}]},{"title":"spring学习笔记(1)-基本概念","date":"2017-08-17T04:43:56.000Z","path":"2017/08/17/Spring学习(1)/","text":"Spring基本概念 Inversion of Control And Dependency InjectionIOCIOC是一种设计的思想，在程序设计时并不直接在某个对象内部去new一个其他对象，而是使用一个IOC容器来创建这些对象。使用IOC容器来控制对象，主要控制外部资源的获取。 所谓inversion，指的就是由容器来帮忙创建及注入依赖对象。依赖对象的获取由主动变为被动，这是一个反转。应用被动的等待IOC容器来创建注入它所需要的资源。 DI应用依赖于IOC容器。因为其需要IOC容器来提供外部资源。IOC将这些外部资源注入某个对象。 其实依赖注入只是一种装配对象的手段，设计的类结构才是基础，如果设计的类结构不支持依赖注入，Spring IoC容器也注入不了任何东西。 循环依赖很显然，循环依赖是指的，当创建A时，需要B，就去创建B，创建B时发现需要A。此时就有问题了，因为A还没创建完成，然后就去新建一个A，这样就会陷入死循环。 Aspect Orient Programming 面向切面编程http://jinnianshilongnian.iteye.com/blog/1418596 在进行OOP开发时，都是基于对组件（比如类）进行开发，然后对组件进行组合，OOP最大问题就是无法解耦组件进行开发，比如我们上边举例，而AOP就是为了克服这个问题而出现的，它来进行这种耦合的分离。AOP为开发者提供一种进行横切关注点。（比如日志关注点横切了支付关注点） 分离并织入的机制，把横切关注点分离，然后通过某种技术织入到系统中，从而无耦合的完成了我们的功能。 关注点：可以认为是所关注的任何东西，比如上边的支付组件；关注点分离：将问题细化从而单独部分，即可以理解为不可再分割的组件，如上边的日志组件和支付组件；横切关注点：一个组件无法完成需要的功能，需要其他组件协作完成，如日志组件横切于支付组件；","tags":[{"name":"spring","slug":"spring","permalink":"http://blog.tangkailin.cn/tags/spring/"},{"name":"java","slug":"java","permalink":"http://blog.tangkailin.cn/tags/java/"}]},{"title":"storm资源汇总","date":"2017-08-10T12:36:59.000Z","path":"2017/08/10/storm资源汇总/","text":"Apache storm 的一些网络资源汇总。 storm入门官网地址 源码地址 storm@twitter 部署教程 Default configuration storm配置项 相关博客 fxjwind的博文，有部分源码解读是老版本的storm。1.0.X的版本之后会有一些变动的地方（如worker如何接收数据）。但作为storm源码阅读的辅助，这些博文还是相当牛逼的。向fxjwind大神致敬。 http://www.cnblogs.com/fxjwind/category/455987.html 徽沪一郎 Blog - Apache Storm 源码走读系列。很cool的博客。 http://www.cnblogs.com/hseagle/p/3756862.html Some important concept Understanding the Internal Message Buffers of Storm Understanding the Parallelism of a Storm Topology Guaranteeing Message Processing At least once storm trident Storm Metrics DRPC详解，中文版译文 storm在zookeeper上的目录结构 kafka原理 storm schedulerstorm 有多个不同的调度器。包括 DefaultScheduler, IsolationScheduler, MultitenantScheduler, ResourceAwareScheduler等。 相关项目 zookeeper thrift，推荐董的博客。 Dynamic Resource Scheduling，该项目支持storm在运行时动态调整资源。 storm on yarn，利用资源调度框架YARN来调度storm资源。 storm on docker,在docker swarm集群里部署storm。 Jstorm，alibaba开源的Java版本及优化版本的类storm流处理系统。 感谢感谢troyding，他的文章让我有了写这篇博客的想法。我的文章是基于他的工作完成的。本文将持续更新。","tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://blog.tangkailin.cn/tags/Distributed-system/"},{"name":"Storm","slug":"Storm","permalink":"http://blog.tangkailin.cn/tags/Storm/"}]}]